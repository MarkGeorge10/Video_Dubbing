2025-03-22 14:34:59,796 - INFO - Training model for language: es
2025-03-22 14:34:59,796 - INFO - Loading and preprocessing data...
2025-03-22 14:35:00,661 - INFO - Cleaned data shape: (118964, 2)
2025-03-22 14:35:00,678 - INFO - First few rows:
  english   target
0     go.      ve.
1     go.    vete.
2     go.    vaya.
3     go.  váyase.
4     hi.    hola.
2025-03-22 14:35:07,179 - INFO - Starting model training...
2025-03-22 14:35:07,235 - INFO - Tokenizing datasets...
2025-03-22 14:35:12,734 - INFO - Training for 10 epochs...
2025-03-22 14:35:14,682 - INFO - Epoch 1, Batch 0, Loss: 4.662381172180176
2025-03-22 14:37:36,186 - INFO - Epoch 1, Batch 100, Loss: 0.07445316016674042
2025-03-22 14:39:57,971 - INFO - Epoch 1, Batch 200, Loss: 0.05574887990951538
2025-03-22 14:42:19,411 - INFO - Epoch 1, Batch 300, Loss: 0.023686697706580162
2025-03-22 14:44:40,905 - INFO - Epoch 1, Batch 400, Loss: 0.0558122955262661
2025-03-22 14:47:02,119 - INFO - Epoch 1, Batch 500, Loss: 0.06813319772481918
2025-03-22 14:49:23,361 - INFO - Epoch 1, Batch 600, Loss: 0.03649597987532616
2025-03-22 14:51:44,958 - INFO - Epoch 1, Batch 700, Loss: 0.02740931697189808
2025-03-22 14:54:06,351 - INFO - Epoch 1, Batch 800, Loss: 0.024024957790970802
2025-03-22 14:56:27,819 - INFO - Epoch 1, Batch 900, Loss: 0.05064420402050018
2025-03-22 14:58:49,551 - INFO - Epoch 1, Batch 1000, Loss: 0.02493305876851082
2025-03-22 15:01:11,161 - INFO - Epoch 1, Batch 1100, Loss: 0.0379183366894722
2025-03-22 15:03:32,640 - INFO - Epoch 1, Batch 1200, Loss: 0.031131122261285782
2025-03-22 15:04:42,088 - INFO - Epoch 1 - Average Training Loss: 0.060409782014787194
2025-03-22 15:15:53,204 - INFO - Epoch 1 - Average Validation Loss: 0.03888392459636237
2025-03-22 15:15:54,652 - INFO - Epoch 2, Batch 0, Loss: 0.05363934487104416
2025-03-22 15:18:16,197 - INFO - Epoch 2, Batch 100, Loss: 0.029529372230172157
2025-03-22 15:20:37,490 - INFO - Epoch 2, Batch 200, Loss: 0.041506968438625336
2025-03-22 15:22:58,805 - INFO - Epoch 2, Batch 300, Loss: 0.021672233939170837
2025-03-22 15:25:20,500 - INFO - Epoch 2, Batch 400, Loss: 0.01925869844853878
2025-03-22 15:27:41,879 - INFO - Epoch 2, Batch 500, Loss: 0.02442072331905365
2025-03-22 15:30:03,404 - INFO - Epoch 2, Batch 600, Loss: 0.01380808837711811
2025-03-22 15:32:24,766 - INFO - Epoch 2, Batch 700, Loss: 0.044939521700143814
2025-03-22 15:34:46,113 - INFO - Epoch 2, Batch 800, Loss: 0.02352154068648815
2025-03-22 15:37:07,307 - INFO - Epoch 2, Batch 900, Loss: 0.03874792531132698
2025-03-22 15:39:28,567 - INFO - Epoch 2, Batch 1000, Loss: 0.02873879112303257
2025-03-22 15:41:50,140 - INFO - Epoch 2, Batch 1100, Loss: 0.030477862805128098
2025-03-22 15:44:11,687 - INFO - Epoch 2, Batch 1200, Loss: 0.02785082347691059
2025-03-22 15:45:21,008 - INFO - Epoch 2 - Average Training Loss: 0.032567072926089166
2025-03-22 15:56:31,809 - INFO - Epoch 2 - Average Validation Loss: 0.03924913886995897
2025-03-22 15:56:33,300 - INFO - Epoch 3, Batch 0, Loss: 0.026515474542975426
2025-03-22 15:58:54,624 - INFO - Epoch 3, Batch 100, Loss: 0.0135755380615592
2025-03-22 16:01:16,058 - INFO - Epoch 3, Batch 200, Loss: 0.03653208538889885
2025-03-22 16:03:37,516 - INFO - Epoch 3, Batch 300, Loss: 0.014633427374064922
2025-03-22 16:05:58,946 - INFO - Epoch 3, Batch 400, Loss: 0.02940811589360237
2025-03-22 16:08:20,457 - INFO - Epoch 3, Batch 500, Loss: 0.017755018547177315
2025-03-22 16:10:41,842 - INFO - Epoch 3, Batch 600, Loss: 0.018494609743356705
2025-03-22 16:13:03,216 - INFO - Epoch 3, Batch 700, Loss: 0.027095498517155647
2025-03-22 16:15:24,796 - INFO - Epoch 3, Batch 800, Loss: 0.02842836268246174
2025-03-22 16:17:46,282 - INFO - Epoch 3, Batch 900, Loss: 0.044689103960990906
2025-03-22 16:20:07,605 - INFO - Epoch 3, Batch 1000, Loss: 0.0222854632884264
2025-03-22 16:22:28,949 - INFO - Epoch 3, Batch 1100, Loss: 0.0373334102332592
2025-03-22 16:24:50,296 - INFO - Epoch 3, Batch 1200, Loss: 0.014367991127073765
2025-03-22 16:25:59,674 - INFO - Epoch 3 - Average Training Loss: 0.025354526549205184
2025-03-22 16:37:10,262 - INFO - Epoch 3 - Average Validation Loss: 0.040969041704745725
2025-03-22 16:37:11,745 - INFO - Epoch 4, Batch 0, Loss: 0.021983541548252106
2025-03-22 16:39:33,157 - INFO - Epoch 4, Batch 100, Loss: 0.015800461173057556
2025-03-22 16:41:54,378 - INFO - Epoch 4, Batch 200, Loss: 0.014592637307941914
2025-03-22 16:44:15,457 - INFO - Epoch 4, Batch 300, Loss: 0.01020714733749628
2025-03-22 16:46:36,617 - INFO - Epoch 4, Batch 400, Loss: 0.023576442152261734
2025-03-22 16:48:57,915 - INFO - Epoch 4, Batch 500, Loss: 0.019085727632045746
2025-03-22 16:51:19,470 - INFO - Epoch 4, Batch 600, Loss: 0.005483264569193125
2025-03-22 16:53:40,938 - INFO - Epoch 4, Batch 700, Loss: 0.051207657903432846
2025-03-22 16:56:02,348 - INFO - Epoch 4, Batch 800, Loss: 0.02790115401148796
2025-03-22 16:58:23,990 - INFO - Epoch 4, Batch 900, Loss: 0.01239064522087574
2025-03-22 17:00:45,357 - INFO - Epoch 4, Batch 1000, Loss: 0.021503807976841927
2025-03-22 17:03:06,501 - INFO - Epoch 4, Batch 1100, Loss: 0.031420912593603134
2025-03-22 17:05:27,846 - INFO - Epoch 4, Batch 1200, Loss: 0.01443556696176529
2025-03-22 17:06:37,348 - INFO - Epoch 4 - Average Training Loss: 0.01957148741595447
2025-03-22 17:17:47,524 - INFO - Epoch 4 - Average Validation Loss: 0.043432081928670846
2025-03-22 17:17:49,005 - INFO - Epoch 5, Batch 0, Loss: 0.016793549060821533
2025-03-22 17:20:10,471 - INFO - Epoch 5, Batch 100, Loss: 0.008535102009773254
2025-03-22 17:22:31,874 - INFO - Epoch 5, Batch 200, Loss: 0.010720789432525635
2025-03-22 17:24:53,558 - INFO - Epoch 5, Batch 300, Loss: 0.011655821464955807
2025-03-22 17:27:15,102 - INFO - Epoch 5, Batch 400, Loss: 0.013977311551570892
2025-03-22 17:29:36,859 - INFO - Epoch 5, Batch 500, Loss: 0.011040281504392624
2025-03-22 17:31:57,859 - INFO - Epoch 5, Batch 600, Loss: 0.016891714185476303
2025-03-22 17:34:19,110 - INFO - Epoch 5, Batch 700, Loss: 0.018542636185884476
2025-03-22 17:36:40,398 - INFO - Epoch 5, Batch 800, Loss: 0.010204853489995003
2025-03-22 17:39:01,613 - INFO - Epoch 5, Batch 900, Loss: 0.013916953466832638
2025-03-22 17:41:22,670 - INFO - Epoch 5, Batch 1000, Loss: 0.025909895077347755
2025-03-22 17:43:43,755 - INFO - Epoch 5, Batch 1100, Loss: 0.0074521922506392
2025-03-22 17:46:04,967 - INFO - Epoch 5, Batch 1200, Loss: 0.016854941844940186
2025-03-22 17:47:14,216 - INFO - Epoch 5 - Average Training Loss: 0.01527605686429888
2025-03-22 17:58:14,164 - INFO - Epoch 5 - Average Validation Loss: 0.046509172992872244
2025-03-22 17:58:15,616 - INFO - Epoch 6, Batch 0, Loss: 0.009129027836024761
2025-03-22 18:00:35,997 - INFO - Epoch 6, Batch 100, Loss: 0.018860217183828354
2025-03-22 18:02:56,314 - INFO - Epoch 6, Batch 200, Loss: 0.012777864001691341
2025-03-22 18:05:16,513 - INFO - Epoch 6, Batch 300, Loss: 0.0058868322521448135
2025-03-22 18:07:37,205 - INFO - Epoch 6, Batch 400, Loss: 0.009570293128490448
2025-03-22 18:09:58,227 - INFO - Epoch 6, Batch 500, Loss: 0.0081653892993927
2025-03-22 18:12:19,124 - INFO - Epoch 6, Batch 600, Loss: 0.020691638812422752
2025-03-22 18:14:40,039 - INFO - Epoch 6, Batch 700, Loss: 0.01123065035790205
2025-03-22 18:17:00,856 - INFO - Epoch 6, Batch 800, Loss: 0.009214118123054504
2025-03-22 18:19:21,636 - INFO - Epoch 6, Batch 900, Loss: 0.007826535031199455
2025-03-22 18:21:42,629 - INFO - Epoch 6, Batch 1000, Loss: 0.01077345386147499
2025-03-22 18:24:03,683 - INFO - Epoch 6, Batch 1100, Loss: 0.02089097537100315
2025-03-22 18:26:24,583 - INFO - Epoch 6, Batch 1200, Loss: 0.011382924392819405
2025-03-22 18:27:33,693 - INFO - Epoch 6 - Average Training Loss: 0.011734297980181873
2025-03-22 18:38:32,708 - INFO - Epoch 6 - Average Validation Loss: 0.049085259069917224
2025-03-22 18:38:34,159 - INFO - Epoch 7, Batch 0, Loss: 0.007322284858673811
2025-03-22 18:40:55,248 - INFO - Epoch 7, Batch 100, Loss: 0.013310601003468037
2025-03-22 18:43:16,241 - INFO - Epoch 7, Batch 200, Loss: 0.0066435681656003
2025-03-22 18:45:37,269 - INFO - Epoch 7, Batch 300, Loss: 0.008243347518146038
2025-03-22 18:47:58,417 - INFO - Epoch 7, Batch 400, Loss: 0.006027794908732176
2025-03-22 18:50:19,529 - INFO - Epoch 7, Batch 500, Loss: 0.00852968916296959
2025-03-22 18:52:40,294 - INFO - Epoch 7, Batch 600, Loss: 0.016170816496014595
2025-03-22 18:55:00,904 - INFO - Epoch 7, Batch 700, Loss: 0.008415394462645054
2025-03-22 18:57:21,201 - INFO - Epoch 7, Batch 800, Loss: 0.005411982070654631
2025-03-22 18:59:41,573 - INFO - Epoch 7, Batch 900, Loss: 0.013255355879664421
2025-03-22 19:02:01,807 - INFO - Epoch 7, Batch 1000, Loss: 0.012593048624694347
2025-03-22 19:04:22,163 - INFO - Epoch 7, Batch 1100, Loss: 0.004404345061630011
2025-03-22 19:06:42,510 - INFO - Epoch 7, Batch 1200, Loss: 0.008020633831620216
2025-03-22 19:07:51,476 - INFO - Epoch 7 - Average Training Loss: 0.009361345788184554
2025-03-22 19:18:47,881 - INFO - Epoch 7 - Average Validation Loss: 0.05252542814506667
2025-03-22 19:18:49,328 - INFO - Epoch 8, Batch 0, Loss: 0.0043830424547195435
2025-03-22 19:21:09,701 - INFO - Epoch 8, Batch 100, Loss: 0.0034914740826934576
2025-03-22 19:23:30,100 - INFO - Epoch 8, Batch 200, Loss: 0.010179232805967331
2025-03-22 19:25:50,377 - INFO - Epoch 8, Batch 300, Loss: 0.006285651121288538
2025-03-22 19:28:10,708 - INFO - Epoch 8, Batch 400, Loss: 0.0031573607120662928
2025-03-22 19:30:31,033 - INFO - Epoch 8, Batch 500, Loss: 0.006696502212435007
2025-03-22 19:32:51,351 - INFO - Epoch 8, Batch 600, Loss: 0.006275468971580267
2025-03-22 19:35:11,355 - INFO - Epoch 8, Batch 700, Loss: 0.0067678759805858135
2025-03-22 19:37:31,677 - INFO - Epoch 8, Batch 800, Loss: 0.008598762564361095
2025-03-22 19:39:52,443 - INFO - Epoch 8, Batch 900, Loss: 0.005633284337818623
2025-03-22 19:42:12,908 - INFO - Epoch 8, Batch 1000, Loss: 0.013253622688353062
2025-03-22 19:44:33,389 - INFO - Epoch 8, Batch 1100, Loss: 0.007381930015981197
2025-03-22 19:46:54,033 - INFO - Epoch 8, Batch 1200, Loss: 0.012088451534509659
2025-03-22 19:48:02,744 - INFO - Epoch 8 - Average Training Loss: 0.00738690572194755
2025-03-22 19:58:59,565 - INFO - Epoch 8 - Average Validation Loss: 0.056271705007344665
2025-03-22 19:59:01,012 - INFO - Epoch 9, Batch 0, Loss: 0.005147881805896759
2025-03-22 20:01:21,586 - INFO - Epoch 9, Batch 100, Loss: 0.009269054979085922
2025-03-22 20:03:41,848 - INFO - Epoch 9, Batch 200, Loss: 0.0014397033955901861
2025-03-22 20:06:02,247 - INFO - Epoch 9, Batch 300, Loss: 0.006295374128967524
2025-03-22 20:08:22,910 - INFO - Epoch 9, Batch 400, Loss: 0.008027561008930206
2025-03-22 20:10:43,434 - INFO - Epoch 9, Batch 500, Loss: 0.008035030215978622
2025-03-22 20:13:04,114 - INFO - Epoch 9, Batch 600, Loss: 0.006764768157154322
2025-03-22 20:15:24,875 - INFO - Epoch 9, Batch 700, Loss: 0.007669287268072367
2025-03-22 20:17:45,528 - INFO - Epoch 9, Batch 800, Loss: 0.005633302964270115
2025-03-22 20:20:06,084 - INFO - Epoch 9, Batch 900, Loss: 0.003012537257745862
2025-03-22 20:22:26,618 - INFO - Epoch 9, Batch 1000, Loss: 0.0029606774915009737
2025-03-22 20:24:47,240 - INFO - Epoch 9, Batch 1100, Loss: 0.003321681171655655
2025-03-22 20:27:08,050 - INFO - Epoch 9, Batch 1200, Loss: 0.00745571032166481
2025-03-22 20:28:17,009 - INFO - Epoch 9 - Average Training Loss: 0.006129576340410859
2025-03-22 20:39:04,627 - INFO - Epoch 9 - Average Validation Loss: 0.058031038425000986
2025-03-22 20:39:06,034 - INFO - Epoch 10, Batch 0, Loss: 0.0033787183929234743
2025-03-22 20:41:23,629 - INFO - Epoch 10, Batch 100, Loss: 0.0013314364477992058
2025-03-22 20:43:41,181 - INFO - Epoch 10, Batch 200, Loss: 0.0077695902436971664
2025-03-22 20:45:58,804 - INFO - Epoch 10, Batch 300, Loss: 0.0015883161686360836
2025-03-22 20:48:16,360 - INFO - Epoch 10, Batch 400, Loss: 0.017886055633425713
2025-03-22 20:50:33,944 - INFO - Epoch 10, Batch 500, Loss: 0.003006888320669532
2025-03-22 20:52:51,548 - INFO - Epoch 10, Batch 600, Loss: 0.002632611896842718
2025-03-22 20:55:09,123 - INFO - Epoch 10, Batch 700, Loss: 0.0035091224126517773
2025-03-22 20:57:26,660 - INFO - Epoch 10, Batch 800, Loss: 0.008204747922718525
2025-03-22 20:59:44,248 - INFO - Epoch 10, Batch 900, Loss: 0.006888232193887234
2025-03-22 21:02:01,791 - INFO - Epoch 10, Batch 1000, Loss: 0.004113527946174145
2025-03-22 21:04:19,326 - INFO - Epoch 10, Batch 1100, Loss: 0.006865421775728464
2025-03-22 21:06:36,964 - INFO - Epoch 10, Batch 1200, Loss: 0.002332657342776656
2025-03-22 21:07:44,356 - INFO - Epoch 10 - Average Training Loss: 0.005303561109909788
2025-03-22 21:18:29,707 - INFO - Epoch 10 - Average Validation Loss: 0.0610575348949254
2025-03-22 21:18:30,481 - INFO - Model and tokenizer saved to /storage/work/mqf5675/Masters/NLP/project/model_output/transformer_en_es/model
2025-03-22 21:18:30,697 - INFO - Training history plot saved.
2025-03-22 21:18:30,700 - INFO - Computing BLEU score...
2025-03-22 21:30:13,067 - INFO - BLEU Score: 0.5336724375930535
2025-03-22 21:30:13,071 - INFO - Performing example translations...
2025-03-22 21:30:13,071 - INFO - Loading model from /storage/work/mqf5675/Masters/NLP/project/model_output/transformer_en_es/model for translation...
2025-03-22 21:30:16,512 - INFO - Translated 'go.' to 'anda.'
2025-03-22 21:30:16,524 - INFO - Loading model from /storage/work/mqf5675/Masters/NLP/project/model_output/transformer_en_es/model for translation...
2025-03-22 21:30:19,143 - INFO - Translated 'hi.' to 'hola.'
2025-03-22 21:30:19,192 - INFO - Training model for language: deu
2025-03-22 21:30:19,192 - INFO - Loading and preprocessing data...
2025-03-22 21:30:20,957 - INFO - Cleaned data shape: (221533, 2)
2025-03-22 21:30:20,958 - INFO - First few rows:
  english     target
0     go.       geh.
1     hi.      hallo
2     hi.  grüß gott
3     run       lauf
4    run.       lauf
2025-03-22 21:30:24,458 - INFO - Starting model training...
2025-03-22 21:30:24,489 - INFO - Tokenizing datasets...
2025-03-22 21:30:31,429 - INFO - Training for 10 epochs...
2025-03-22 21:30:32,735 - INFO - Epoch 1, Batch 0, Loss: 8.051936149597168
2025-03-22 21:32:43,829 - INFO - Epoch 1, Batch 100, Loss: 0.08612062782049179
2025-03-22 21:34:54,801 - INFO - Epoch 1, Batch 200, Loss: 0.08154548704624176
2025-03-22 21:37:05,803 - INFO - Epoch 1, Batch 300, Loss: 0.10175551474094391
2025-03-22 21:39:16,834 - INFO - Epoch 1, Batch 400, Loss: 0.05279049649834633
2025-03-22 21:41:28,380 - INFO - Epoch 1, Batch 500, Loss: 0.06844649463891983
2025-03-22 21:43:40,927 - INFO - Epoch 1, Batch 600, Loss: 0.063968226313591
2025-03-22 21:45:53,533 - INFO - Epoch 1, Batch 700, Loss: 0.08892740309238434
2025-03-22 21:48:06,087 - INFO - Epoch 1, Batch 800, Loss: 0.06435506045818329
2025-03-22 21:50:18,631 - INFO - Epoch 1, Batch 900, Loss: 0.07729247212409973
2025-03-22 21:52:31,209 - INFO - Epoch 1, Batch 1000, Loss: 0.04297702759504318
2025-03-22 21:54:43,752 - INFO - Epoch 1, Batch 1100, Loss: 0.06751309335231781
2025-03-22 21:56:56,324 - INFO - Epoch 1, Batch 1200, Loss: 0.0850670263171196
2025-03-22 21:58:01,167 - INFO - Epoch 1 - Average Training Loss: 0.10786147004887461
2025-03-22 22:17:17,576 - INFO - Epoch 1 - Average Validation Loss: 0.05889950208349106
2025-03-22 22:17:18,939 - INFO - Epoch 2, Batch 0, Loss: 0.03433799743652344
2025-03-22 22:19:31,464 - INFO - Epoch 2, Batch 100, Loss: 0.030021457001566887
2025-03-22 22:21:44,029 - INFO - Epoch 2, Batch 200, Loss: 0.03951949253678322
2025-03-22 22:23:56,508 - INFO - Epoch 2, Batch 300, Loss: 0.043369635939598083
2025-03-22 22:26:08,919 - INFO - Epoch 2, Batch 400, Loss: 0.05547691881656647
2025-03-22 22:28:21,426 - INFO - Epoch 2, Batch 500, Loss: 0.06961783766746521
2025-03-22 22:30:33,733 - INFO - Epoch 2, Batch 600, Loss: 0.039398375898599625
2025-03-22 22:32:46,048 - INFO - Epoch 2, Batch 700, Loss: 0.07831781357526779
2025-03-22 22:34:58,372 - INFO - Epoch 2, Batch 800, Loss: 0.07210074365139008
2025-03-22 22:37:10,664 - INFO - Epoch 2, Batch 900, Loss: 0.0587385855615139
2025-03-22 22:39:22,875 - INFO - Epoch 2, Batch 1000, Loss: 0.0522252693772316
2025-03-22 22:41:35,099 - INFO - Epoch 2, Batch 1100, Loss: 0.09208639711141586
2025-03-22 22:43:47,345 - INFO - Epoch 2, Batch 1200, Loss: 0.0521673858165741
2025-03-22 22:44:52,045 - INFO - Epoch 2 - Average Training Loss: 0.053372727516293524
2025-03-22 23:04:06,027 - INFO - Epoch 2 - Average Validation Loss: 0.056404441286597926
2025-03-22 23:04:07,345 - INFO - Epoch 3, Batch 0, Loss: 0.03973532095551491
2025-03-22 23:06:19,156 - INFO - Epoch 3, Batch 100, Loss: 0.10339459031820297
2025-03-22 23:08:31,054 - INFO - Epoch 3, Batch 200, Loss: 0.04223322123289108
2025-03-22 23:10:42,888 - INFO - Epoch 3, Batch 300, Loss: 0.06189518794417381
2025-03-22 23:12:54,748 - INFO - Epoch 3, Batch 400, Loss: 0.03799434378743172
2025-03-22 23:15:06,676 - INFO - Epoch 3, Batch 500, Loss: 0.051912229508161545
2025-03-22 23:17:18,524 - INFO - Epoch 3, Batch 600, Loss: 0.08304595202207565
2025-03-22 23:19:30,368 - INFO - Epoch 3, Batch 700, Loss: 0.055332984775304794
2025-03-22 23:21:42,217 - INFO - Epoch 3, Batch 800, Loss: 0.019892409443855286
2025-03-22 23:23:54,076 - INFO - Epoch 3, Batch 900, Loss: 0.03599044680595398
2025-03-22 23:26:05,908 - INFO - Epoch 3, Batch 1000, Loss: 0.04105426371097565
2025-03-22 23:28:17,761 - INFO - Epoch 3, Batch 1100, Loss: 0.07575718313455582
2025-03-22 23:30:29,599 - INFO - Epoch 3, Batch 1200, Loss: 0.03429609164595604
2025-03-22 23:31:34,185 - INFO - Epoch 3 - Average Training Loss: 0.04220938428863883
2025-03-22 23:50:42,904 - INFO - Epoch 3 - Average Validation Loss: 0.05668884643747737
2025-03-22 23:50:44,229 - INFO - Epoch 4, Batch 0, Loss: 0.060562919825315475
2025-03-22 23:52:56,044 - INFO - Epoch 4, Batch 100, Loss: 0.029612403362989426
2025-03-22 23:55:07,855 - INFO - Epoch 4, Batch 200, Loss: 0.030288366600871086
2025-03-22 23:57:19,713 - INFO - Epoch 4, Batch 300, Loss: 0.03467562049627304
2025-03-22 23:59:31,516 - INFO - Epoch 4, Batch 400, Loss: 0.034703329205513
2025-03-23 00:01:43,362 - INFO - Epoch 4, Batch 500, Loss: 0.019872017204761505
2025-03-23 00:03:55,168 - INFO - Epoch 4, Batch 600, Loss: 0.025631273165345192
2025-03-23 00:06:06,983 - INFO - Epoch 4, Batch 700, Loss: 0.04285731166601181
2025-03-23 00:08:18,752 - INFO - Epoch 4, Batch 800, Loss: 0.023121042177081108
2025-03-23 00:10:30,612 - INFO - Epoch 4, Batch 900, Loss: 0.032528217881917953
2025-03-23 00:12:42,435 - INFO - Epoch 4, Batch 1000, Loss: 0.039671167731285095
2025-03-23 00:14:54,240 - INFO - Epoch 4, Batch 1100, Loss: 0.04358594864606857
2025-03-23 00:17:06,075 - INFO - Epoch 4, Batch 1200, Loss: 0.039184875786304474
2025-03-23 00:18:10,667 - INFO - Epoch 4 - Average Training Loss: 0.033711295806989075
2025-03-23 00:37:24,016 - INFO - Epoch 4 - Average Validation Loss: 0.058050223657895396
2025-03-23 00:37:25,345 - INFO - Epoch 5, Batch 0, Loss: 0.039019547402858734
2025-03-23 00:39:37,155 - INFO - Epoch 5, Batch 100, Loss: 0.016893358901143074
2025-03-23 00:41:48,988 - INFO - Epoch 5, Batch 200, Loss: 0.017171276733279228
2025-03-23 00:44:00,826 - INFO - Epoch 5, Batch 300, Loss: 0.02172398567199707
2025-03-23 00:46:12,614 - INFO - Epoch 5, Batch 400, Loss: 0.01681305468082428
2025-03-23 00:48:24,415 - INFO - Epoch 5, Batch 500, Loss: 0.02792218327522278
2025-03-23 00:50:36,187 - INFO - Epoch 5, Batch 600, Loss: 0.020254475995898247
2025-03-23 00:52:48,001 - INFO - Epoch 5, Batch 700, Loss: 0.024492936208844185
2025-03-23 00:54:59,784 - INFO - Epoch 5, Batch 800, Loss: 0.02492309920489788
2025-03-23 00:57:11,548 - INFO - Epoch 5, Batch 900, Loss: 0.021725857630372047
2025-03-23 00:59:23,341 - INFO - Epoch 5, Batch 1000, Loss: 0.02460353821516037
2025-03-23 01:01:35,126 - INFO - Epoch 5, Batch 1100, Loss: 0.03666254132986069
2025-03-23 01:03:46,964 - INFO - Epoch 5, Batch 1200, Loss: 0.02979252301156521
2025-03-23 01:04:51,680 - INFO - Epoch 5 - Average Training Loss: 0.026854121885821224
2025-03-23 01:24:02,951 - INFO - Epoch 5 - Average Validation Loss: 0.060391113841137094
2025-03-23 01:24:04,274 - INFO - Epoch 6, Batch 0, Loss: 0.020116625353693962
2025-03-23 01:26:16,072 - INFO - Epoch 6, Batch 100, Loss: 0.014608104713261127
2025-03-23 01:28:27,833 - INFO - Epoch 6, Batch 200, Loss: 0.02496630884706974
2025-03-23 01:30:39,607 - INFO - Epoch 6, Batch 300, Loss: 0.016440652310848236
2025-03-23 01:32:51,393 - INFO - Epoch 6, Batch 400, Loss: 0.0180849377065897
2025-03-23 01:35:03,208 - INFO - Epoch 6, Batch 500, Loss: 0.033232733607292175
2025-03-23 01:37:14,992 - INFO - Epoch 6, Batch 600, Loss: 0.012990359216928482
2025-03-23 01:39:26,821 - INFO - Epoch 6, Batch 700, Loss: 0.014883918687701225
2025-03-23 01:41:38,630 - INFO - Epoch 6, Batch 800, Loss: 0.011631617322564125
2025-03-23 01:43:50,475 - INFO - Epoch 6, Batch 900, Loss: 0.01414759736508131
2025-03-23 01:46:02,286 - INFO - Epoch 6, Batch 1000, Loss: 0.01969621703028679
2025-03-23 01:48:14,074 - INFO - Epoch 6, Batch 1100, Loss: 0.011397444643080235
2025-03-23 01:50:25,881 - INFO - Epoch 6, Batch 1200, Loss: 0.010413226671516895
2025-03-23 01:51:30,567 - INFO - Epoch 6 - Average Training Loss: 0.020984653836861253
2025-03-23 02:10:40,593 - INFO - Epoch 6 - Average Validation Loss: 0.0638621982529029
2025-03-23 02:10:41,917 - INFO - Epoch 7, Batch 0, Loss: 0.025514263659715652
2025-03-23 02:12:53,722 - INFO - Epoch 7, Batch 100, Loss: 0.012146905064582825
2025-03-23 02:15:05,556 - INFO - Epoch 7, Batch 200, Loss: 0.01803777925670147
2025-03-23 02:17:17,385 - INFO - Epoch 7, Batch 300, Loss: 0.008090878836810589
2025-03-23 02:19:29,209 - INFO - Epoch 7, Batch 400, Loss: 0.013131706044077873
2025-03-23 02:21:41,041 - INFO - Epoch 7, Batch 500, Loss: 0.011835952289402485
2025-03-23 02:23:52,859 - INFO - Epoch 7, Batch 600, Loss: 0.011384387500584126
2025-03-23 02:26:04,686 - INFO - Epoch 7, Batch 700, Loss: 0.013083775527775288
2025-03-23 02:28:16,470 - INFO - Epoch 7, Batch 800, Loss: 0.017278581857681274
2025-03-23 02:30:28,301 - INFO - Epoch 7, Batch 900, Loss: 0.008427354507148266
2025-03-23 02:32:40,057 - INFO - Epoch 7, Batch 1000, Loss: 0.010867455042898655
2025-03-23 02:34:51,898 - INFO - Epoch 7, Batch 1100, Loss: 0.020203102380037308
2025-03-23 02:37:03,679 - INFO - Epoch 7, Batch 1200, Loss: 0.01718493551015854
2025-03-23 02:38:08,299 - INFO - Epoch 7 - Average Training Loss: 0.016541985906846822
2025-03-23 02:57:20,415 - INFO - Epoch 7 - Average Validation Loss: 0.06623711026837778
2025-03-23 02:57:21,731 - INFO - Epoch 8, Batch 0, Loss: 0.010796383023262024
2025-03-23 02:59:33,488 - INFO - Epoch 8, Batch 100, Loss: 0.008704864419996738
2025-03-23 03:01:45,307 - INFO - Epoch 8, Batch 200, Loss: 0.008831801824271679
2025-03-23 03:03:57,148 - INFO - Epoch 8, Batch 300, Loss: 0.012248130515217781
2025-03-23 03:06:09,414 - INFO - Epoch 8, Batch 400, Loss: 0.019497092813253403
2025-03-23 03:08:21,228 - INFO - Epoch 8, Batch 500, Loss: 0.009824338369071484
2025-03-23 03:10:33,006 - INFO - Epoch 8, Batch 600, Loss: 0.011999204754829407
2025-03-23 03:12:44,821 - INFO - Epoch 8, Batch 700, Loss: 0.016365474089980125
2025-03-23 03:14:56,590 - INFO - Epoch 8, Batch 800, Loss: 0.014231602661311626
2025-03-23 03:17:08,521 - INFO - Epoch 8, Batch 900, Loss: 0.014729627408087254
2025-03-23 03:19:20,288 - INFO - Epoch 8, Batch 1000, Loss: 0.01552365068346262
2025-03-23 03:21:32,052 - INFO - Epoch 8, Batch 1100, Loss: 0.02542893961071968
2025-03-23 03:23:43,843 - INFO - Epoch 8, Batch 1200, Loss: 0.011714565567672253
2025-03-23 03:24:48,402 - INFO - Epoch 8 - Average Training Loss: 0.01321785638500005
2025-03-23 03:44:01,637 - INFO - Epoch 8 - Average Validation Loss: 0.0696804879825089
2025-03-23 03:44:02,955 - INFO - Epoch 9, Batch 0, Loss: 0.009814845398068428
2025-03-23 03:46:14,755 - INFO - Epoch 9, Batch 100, Loss: 0.008463826961815357
2025-03-23 03:48:26,773 - INFO - Epoch 9, Batch 200, Loss: 0.00389733980409801
2025-03-23 03:50:38,553 - INFO - Epoch 9, Batch 300, Loss: 0.01755482517182827
2025-03-23 03:52:50,342 - INFO - Epoch 9, Batch 400, Loss: 0.015004731714725494
2025-03-23 03:55:02,112 - INFO - Epoch 9, Batch 500, Loss: 0.013019104488193989
2025-03-23 03:57:13,882 - INFO - Epoch 9, Batch 600, Loss: 0.0068076555617153645
2025-03-23 03:59:25,748 - INFO - Epoch 9, Batch 700, Loss: 0.006894747260957956
2025-03-23 04:01:37,545 - INFO - Epoch 9, Batch 800, Loss: 0.010619155131280422
2025-03-23 04:03:49,362 - INFO - Epoch 9, Batch 900, Loss: 0.017646804451942444
2025-03-23 04:06:01,133 - INFO - Epoch 9, Batch 1000, Loss: 0.010837607085704803
2025-03-23 04:08:12,983 - INFO - Epoch 9, Batch 1100, Loss: 0.01223053503781557
2025-03-23 04:10:24,784 - INFO - Epoch 9, Batch 1200, Loss: 0.013814043253660202
2025-03-23 04:11:29,216 - INFO - Epoch 9 - Average Training Loss: 0.010610179041326046
2025-03-23 04:30:43,481 - INFO - Epoch 9 - Average Validation Loss: 0.07214211139171081
2025-03-23 04:30:44,802 - INFO - Epoch 10, Batch 0, Loss: 0.012987312860786915
2025-03-23 04:32:56,616 - INFO - Epoch 10, Batch 100, Loss: 0.004287238232791424
2025-03-23 04:35:08,367 - INFO - Epoch 10, Batch 200, Loss: 0.00977403111755848
2025-03-23 04:37:20,134 - INFO - Epoch 10, Batch 300, Loss: 0.005661086179316044
2025-03-23 04:39:31,931 - INFO - Epoch 10, Batch 400, Loss: 0.005299518350511789
2025-03-23 04:41:43,690 - INFO - Epoch 10, Batch 500, Loss: 0.007293710485100746
2025-03-23 04:43:55,527 - INFO - Epoch 10, Batch 600, Loss: 0.0069936830550432205
2025-03-23 04:46:07,286 - INFO - Epoch 10, Batch 700, Loss: 0.0058874827809631824
2025-03-23 04:48:19,120 - INFO - Epoch 10, Batch 800, Loss: 0.011555051431059837
2025-03-23 04:50:30,900 - INFO - Epoch 10, Batch 900, Loss: 0.008855556137859821
2025-03-23 04:52:42,675 - INFO - Epoch 10, Batch 1000, Loss: 0.0029530159663408995
2025-03-23 04:54:54,477 - INFO - Epoch 10, Batch 1100, Loss: 0.008504408411681652
2025-03-23 04:57:06,283 - INFO - Epoch 10, Batch 1200, Loss: 0.009625595062971115
2025-03-23 04:58:10,795 - INFO - Epoch 10 - Average Training Loss: 0.008714718777593226
2025-03-23 05:17:11,265 - INFO - Epoch 10 - Average Validation Loss: 0.07480469810012595
2025-03-23 05:17:12,008 - INFO - Model and tokenizer saved to /storage/work/mqf5675/Masters/NLP/project/model_output/transformer_en_deu/model
2025-03-23 05:17:12,132 - INFO - Training history plot saved.
2025-03-23 05:17:12,135 - INFO - Computing BLEU score...
2025-03-23 05:39:51,532 - INFO - BLEU Score: 0.429077236579097
2025-03-23 05:39:51,537 - INFO - Performing example translations...
2025-03-23 05:39:51,537 - INFO - Loading model from /storage/work/mqf5675/Masters/NLP/project/model_output/transformer_en_deu/model for translation...
2025-03-23 05:39:55,009 - INFO - Translated 'go.' to 'geht.'
2025-03-23 05:39:55,021 - INFO - Loading model from /storage/work/mqf5675/Masters/NLP/project/model_output/transformer_en_deu/model for translation...
2025-03-23 05:39:57,557 - INFO - Translated 'hi.' to 'hallo'
2025-03-23 05:39:57,615 - INFO - Training model for language: ita
2025-03-23 05:39:57,615 - INFO - Loading and preprocessing data...
2025-03-23 05:40:00,256 - INFO - Cleaned data shape: (364199, 2)
2025-03-23 05:40:00,257 - INFO - First few rows:
  english   target
0     hi.     ciao
1     hi.    ciao.
2     run    corri
3     run    corra
4     run  correte
2025-03-23 05:40:04,801 - INFO - Starting model training...
2025-03-23 05:40:04,839 - INFO - Tokenizing datasets...
2025-03-23 05:40:14,180 - INFO - Training for 10 epochs...
2025-03-23 05:40:15,653 - INFO - Epoch 1, Batch 0, Loss: 9.772632598876953
2025-03-23 05:42:46,195 - INFO - Epoch 1, Batch 100, Loss: 0.09123218059539795
2025-03-23 05:45:16,628 - INFO - Epoch 1, Batch 200, Loss: 0.06689628213644028
2025-03-23 05:47:47,101 - INFO - Epoch 1, Batch 300, Loss: 0.058184914290905
2025-03-23 05:50:17,551 - INFO - Epoch 1, Batch 400, Loss: 0.02987781912088394
2025-03-23 05:52:47,952 - INFO - Epoch 1, Batch 500, Loss: 0.055964600294828415
2025-03-23 05:55:18,436 - INFO - Epoch 1, Batch 600, Loss: 0.03441601246595383
2025-03-23 05:57:48,864 - INFO - Epoch 1, Batch 700, Loss: 0.02941080369055271
2025-03-23 06:00:19,311 - INFO - Epoch 1, Batch 800, Loss: 0.03427073359489441
2025-03-23 06:02:49,756 - INFO - Epoch 1, Batch 900, Loss: 0.031203780323266983
2025-03-23 06:05:20,117 - INFO - Epoch 1, Batch 1000, Loss: 0.030606113374233246
2025-03-23 06:07:50,565 - INFO - Epoch 1, Batch 1100, Loss: 0.012173742987215519
2025-03-23 06:10:21,053 - INFO - Epoch 1, Batch 1200, Loss: 0.020695660263299942
2025-03-23 06:11:34,773 - INFO - Epoch 1 - Average Training Loss: 0.09367369239032268
2025-03-23 06:47:53,874 - INFO - Epoch 1 - Average Validation Loss: 0.029220466288254598
2025-03-23 06:47:55,394 - INFO - Epoch 2, Batch 0, Loss: 0.012561431154608727
2025-03-23 06:50:25,841 - INFO - Epoch 2, Batch 100, Loss: 0.02091270312666893
2025-03-23 06:52:56,354 - INFO - Epoch 2, Batch 200, Loss: 0.01749410293996334
2025-03-23 06:55:26,825 - INFO - Epoch 2, Batch 300, Loss: 0.019483283162117004
2025-03-23 06:57:57,275 - INFO - Epoch 2, Batch 400, Loss: 0.039090294390916824
2025-03-23 07:00:27,727 - INFO - Epoch 2, Batch 500, Loss: 0.04266943410038948
2025-03-23 07:02:58,189 - INFO - Epoch 2, Batch 600, Loss: 0.017588160932064056
2025-03-23 07:05:28,621 - INFO - Epoch 2, Batch 700, Loss: 0.024844935163855553
2025-03-23 07:07:59,074 - INFO - Epoch 2, Batch 800, Loss: 0.02608765847980976
2025-03-23 07:10:29,492 - INFO - Epoch 2, Batch 900, Loss: 0.015245954506099224
2025-03-23 07:12:59,959 - INFO - Epoch 2, Batch 1000, Loss: 0.021351605653762817
2025-03-23 07:15:30,424 - INFO - Epoch 2, Batch 1100, Loss: 0.04452419653534889
2025-03-23 07:18:00,907 - INFO - Epoch 2, Batch 1200, Loss: 0.017243165522813797
2025-03-23 07:19:14,623 - INFO - Epoch 2 - Average Training Loss: 0.02558947980478406
2025-03-23 07:55:41,092 - INFO - Epoch 2 - Average Validation Loss: 0.02761401901452794
2025-03-23 07:55:42,599 - INFO - Epoch 3, Batch 0, Loss: 0.018343497067689896
2025-03-23 07:58:12,916 - INFO - Epoch 3, Batch 100, Loss: 0.01982049271464348
2025-03-23 08:00:43,256 - INFO - Epoch 3, Batch 200, Loss: 0.02204286865890026
2025-03-23 08:03:13,598 - INFO - Epoch 3, Batch 300, Loss: 0.018453586846590042
2025-03-23 08:05:43,938 - INFO - Epoch 3, Batch 400, Loss: 0.02299431525170803
2025-03-23 08:08:14,278 - INFO - Epoch 3, Batch 500, Loss: 0.012577177956700325
2025-03-23 08:10:44,616 - INFO - Epoch 3, Batch 600, Loss: 0.012588600628077984
2025-03-23 08:13:14,968 - INFO - Epoch 3, Batch 700, Loss: 0.016278035938739777
2025-03-23 08:15:45,313 - INFO - Epoch 3, Batch 800, Loss: 0.02062063477933407
2025-03-23 08:18:15,663 - INFO - Epoch 3, Batch 900, Loss: 0.01926528662443161
2025-03-23 08:20:45,995 - INFO - Epoch 3, Batch 1000, Loss: 0.014478297904133797
2025-03-23 08:23:16,324 - INFO - Epoch 3, Batch 1100, Loss: 0.02032577060163021
2025-03-23 08:25:46,681 - INFO - Epoch 3, Batch 1200, Loss: 0.018139027059078217
2025-03-23 08:27:00,367 - INFO - Epoch 3 - Average Training Loss: 0.02038249038942158
2025-03-23 09:03:22,176 - INFO - Epoch 3 - Average Validation Loss: 0.02789397315493857
2025-03-23 09:03:23,685 - INFO - Epoch 4, Batch 0, Loss: 0.012682167813181877
2025-03-23 09:05:54,010 - INFO - Epoch 4, Batch 100, Loss: 0.0189819298684597
2025-03-23 09:08:24,337 - INFO - Epoch 4, Batch 200, Loss: 0.008642119355499744
2025-03-23 09:10:54,702 - INFO - Epoch 4, Batch 300, Loss: 0.016414092853665352
2025-03-23 09:13:25,046 - INFO - Epoch 4, Batch 400, Loss: 0.014238973148167133
2025-03-23 09:15:55,377 - INFO - Epoch 4, Batch 500, Loss: 0.011865141801536083
2025-03-23 09:18:25,733 - INFO - Epoch 4, Batch 600, Loss: 0.016694460064172745
2025-03-23 09:20:56,059 - INFO - Epoch 4, Batch 700, Loss: 0.01740761660039425
2025-03-23 09:23:26,385 - INFO - Epoch 4, Batch 800, Loss: 0.011647054925560951
2025-03-23 09:25:56,738 - INFO - Epoch 4, Batch 900, Loss: 0.02095003053545952
2025-03-23 09:28:27,072 - INFO - Epoch 4, Batch 1000, Loss: 0.014659039676189423
2025-03-23 09:30:57,399 - INFO - Epoch 4, Batch 1100, Loss: 0.026604125276207924
2025-03-23 09:33:27,717 - INFO - Epoch 4, Batch 1200, Loss: 0.03227321803569794
2025-03-23 09:34:41,400 - INFO - Epoch 4 - Average Training Loss: 0.01651525964811444
2025-03-23 10:11:06,964 - INFO - Epoch 4 - Average Validation Loss: 0.029086850556347976
2025-03-23 10:11:08,472 - INFO - Epoch 5, Batch 0, Loss: 0.01808466762304306
2025-03-23 10:13:38,813 - INFO - Epoch 5, Batch 100, Loss: 0.012404072098433971
2025-03-23 10:16:09,151 - INFO - Epoch 5, Batch 200, Loss: 0.012091405689716339
2025-03-23 10:18:39,497 - INFO - Epoch 5, Batch 300, Loss: 0.011018279939889908
2025-03-23 10:21:09,850 - INFO - Epoch 5, Batch 400, Loss: 0.00522641884163022
2025-03-23 10:23:40,211 - INFO - Epoch 5, Batch 500, Loss: 0.009110539220273495
2025-03-23 10:26:10,549 - INFO - Epoch 5, Batch 600, Loss: 0.01797262206673622
2025-03-23 10:28:40,906 - INFO - Epoch 5, Batch 700, Loss: 0.011828913353383541
2025-03-23 10:31:11,232 - INFO - Epoch 5, Batch 800, Loss: 0.014839873649179935
2025-03-23 10:33:41,637 - INFO - Epoch 5, Batch 900, Loss: 0.023991171270608902
2025-03-23 10:36:11,987 - INFO - Epoch 5, Batch 1000, Loss: 0.010858425870537758
2025-03-23 10:38:42,399 - INFO - Epoch 5, Batch 1100, Loss: 0.012344449758529663
2025-03-23 10:41:12,753 - INFO - Epoch 5, Batch 1200, Loss: 0.01384888868778944
2025-03-23 10:42:26,413 - INFO - Epoch 5 - Average Training Loss: 0.013710314280167222
2025-03-23 11:18:51,983 - INFO - Epoch 5 - Average Validation Loss: 0.0302641292037383
2025-03-23 11:18:53,490 - INFO - Epoch 6, Batch 0, Loss: 0.011472618207335472
2025-03-23 11:21:23,841 - INFO - Epoch 6, Batch 100, Loss: 0.012350091710686684
2025-03-23 11:23:54,177 - INFO - Epoch 6, Batch 200, Loss: 0.009594948962330818
2025-03-23 11:26:24,549 - INFO - Epoch 6, Batch 300, Loss: 0.009470799937844276
2025-03-23 11:28:54,909 - INFO - Epoch 6, Batch 400, Loss: 0.009275977499783039
2025-03-23 11:31:25,301 - INFO - Epoch 6, Batch 500, Loss: 0.01271065790206194
2025-03-23 11:33:55,671 - INFO - Epoch 6, Batch 600, Loss: 0.014970729127526283
2025-03-23 11:36:26,051 - INFO - Epoch 6, Batch 700, Loss: 0.007860049605369568
2025-03-23 11:38:56,557 - INFO - Epoch 6, Batch 800, Loss: 0.012495185248553753
2025-03-23 11:41:27,143 - INFO - Epoch 6, Batch 900, Loss: 0.009327982552349567
2025-03-23 11:43:57,767 - INFO - Epoch 6, Batch 1000, Loss: 0.015632180497050285
2025-03-23 11:46:28,385 - INFO - Epoch 6, Batch 1100, Loss: 0.008908300660550594
2025-03-23 11:48:59,012 - INFO - Epoch 6, Batch 1200, Loss: 0.021275870501995087
2025-03-23 11:50:12,824 - INFO - Epoch 6 - Average Training Loss: 0.011578169232606888
2025-03-23 12:26:40,785 - INFO - Epoch 6 - Average Validation Loss: 0.03214666797866844
2025-03-23 12:26:42,295 - INFO - Epoch 7, Batch 0, Loss: 0.008972096256911755
2025-03-23 12:29:12,932 - INFO - Epoch 7, Batch 100, Loss: 0.0058927275240421295
2025-03-23 12:31:43,547 - INFO - Epoch 7, Batch 200, Loss: 0.005955362692475319
2025-03-23 12:34:14,148 - INFO - Epoch 7, Batch 300, Loss: 0.010120234452188015
2025-03-23 12:36:44,479 - INFO - Epoch 7, Batch 400, Loss: 0.011748811230063438
2025-03-23 12:39:14,903 - INFO - Epoch 7, Batch 500, Loss: 0.012176482938230038
2025-03-23 12:41:45,276 - INFO - Epoch 7, Batch 600, Loss: 0.01196651067584753
2025-03-23 12:44:15,666 - INFO - Epoch 7, Batch 700, Loss: 0.012950906530022621
2025-03-23 12:46:46,031 - INFO - Epoch 7, Batch 800, Loss: 0.010917862877249718
2025-03-23 12:49:16,384 - INFO - Epoch 7, Batch 900, Loss: 0.00913095660507679
2025-03-23 12:51:46,733 - INFO - Epoch 7, Batch 1000, Loss: 0.01701819710433483
2025-03-23 12:54:17,093 - INFO - Epoch 7, Batch 1100, Loss: 0.013610168360173702
2025-03-23 12:56:47,731 - INFO - Epoch 7, Batch 1200, Loss: 0.00990260113030672
2025-03-23 12:58:01,805 - INFO - Epoch 7 - Average Training Loss: 0.009835954374074935
2025-03-23 13:34:44,330 - INFO - Epoch 7 - Average Validation Loss: 0.03374190572849701
2025-03-23 13:34:45,849 - INFO - Epoch 8, Batch 0, Loss: 0.007072866894304752
2025-03-23 13:37:17,181 - INFO - Epoch 8, Batch 100, Loss: 0.005373949650675058
2025-03-23 13:39:48,313 - INFO - Epoch 8, Batch 200, Loss: 0.0072601852007210255
2025-03-23 13:42:19,406 - INFO - Epoch 8, Batch 300, Loss: 0.007154080085456371
2025-03-23 13:44:53,951 - INFO - Epoch 8, Batch 400, Loss: 0.006778656970709562
2025-03-23 13:47:28,742 - INFO - Epoch 8, Batch 500, Loss: 0.009665646590292454
2025-03-23 13:50:03,449 - INFO - Epoch 8, Batch 600, Loss: 0.010173049755394459
2025-03-23 13:52:38,098 - INFO - Epoch 8, Batch 700, Loss: 0.019669445231556892
2025-03-23 13:55:11,929 - INFO - Epoch 8, Batch 800, Loss: 0.007628387771546841
2025-03-23 13:57:46,478 - INFO - Epoch 8, Batch 900, Loss: 0.006618835497647524
2025-03-23 14:00:21,619 - INFO - Epoch 8, Batch 1000, Loss: 0.012642832472920418
2025-03-23 14:02:56,892 - INFO - Epoch 8, Batch 1100, Loss: 0.014811187982559204
2025-03-23 14:05:32,024 - INFO - Epoch 8, Batch 1200, Loss: 0.004975142423063517
2025-03-23 14:06:48,036 - INFO - Epoch 8 - Average Training Loss: 0.008421766841970385
2025-03-23 14:43:29,188 - INFO - Epoch 8 - Average Validation Loss: 0.03555640766723173
2025-03-23 14:43:30,701 - INFO - Epoch 9, Batch 0, Loss: 0.006530491169542074
2025-03-23 14:46:01,688 - INFO - Epoch 9, Batch 100, Loss: 0.008477108553051949
2025-03-23 14:48:32,658 - INFO - Epoch 9, Batch 200, Loss: 0.006121666170656681
2025-03-23 14:51:03,642 - INFO - Epoch 9, Batch 300, Loss: 0.012716993689537048
2025-03-23 14:53:34,631 - INFO - Epoch 9, Batch 400, Loss: 0.008003412745893002
2025-03-23 14:56:05,608 - INFO - Epoch 9, Batch 500, Loss: 0.005590249318629503
2025-03-23 14:58:36,560 - INFO - Epoch 9, Batch 600, Loss: 0.00467271264642477
2025-03-23 15:01:07,488 - INFO - Epoch 9, Batch 700, Loss: 0.008851108141243458
2025-03-23 15:03:38,437 - INFO - Epoch 9, Batch 800, Loss: 0.0016908359248191118
2025-03-23 15:06:09,416 - INFO - Epoch 9, Batch 900, Loss: 0.006068121176213026
2025-03-23 15:08:40,442 - INFO - Epoch 9, Batch 1000, Loss: 0.009341618046164513
2025-03-23 15:11:11,497 - INFO - Epoch 9, Batch 1100, Loss: 0.004071811214089394
2025-03-23 15:13:42,562 - INFO - Epoch 9, Batch 1200, Loss: 0.0075829667039215565
2025-03-23 15:14:56,595 - INFO - Epoch 9 - Average Training Loss: 0.007341667091660202
2025-03-23 15:51:34,519 - INFO - Epoch 9 - Average Validation Loss: 0.03726547371173495
2025-03-23 15:51:36,051 - INFO - Epoch 10, Batch 0, Loss: 0.0033455602824687958
2025-03-23 15:54:07,865 - INFO - Epoch 10, Batch 100, Loss: 0.008374550379812717
2025-03-23 15:56:40,070 - INFO - Epoch 10, Batch 200, Loss: 0.005387396086007357
2025-03-23 15:59:11,807 - INFO - Epoch 10, Batch 300, Loss: 0.0061780125834047794
2025-03-23 16:01:43,319 - INFO - Epoch 10, Batch 400, Loss: 0.005439885426312685
2025-03-23 16:04:15,108 - INFO - Epoch 10, Batch 500, Loss: 0.003344407770782709
2025-03-23 16:06:46,762 - INFO - Epoch 10, Batch 600, Loss: 0.009485408663749695
2025-03-23 16:09:18,462 - INFO - Epoch 10, Batch 700, Loss: 0.007393182255327702
2025-03-23 16:11:48,868 - INFO - Epoch 10, Batch 800, Loss: 0.004608619026839733
2025-03-23 16:14:19,173 - INFO - Epoch 10, Batch 900, Loss: 0.003945322707295418
2025-03-23 16:16:49,503 - INFO - Epoch 10, Batch 1000, Loss: 0.005401233211159706
2025-03-23 16:19:19,827 - INFO - Epoch 10, Batch 1100, Loss: 0.0020203047897666693
2025-03-23 16:21:50,141 - INFO - Epoch 10, Batch 1200, Loss: 0.0032330909743905067
2025-03-23 16:23:03,791 - INFO - Epoch 10 - Average Training Loss: 0.00632839898923412
2025-03-23 17:00:32,827 - INFO - Epoch 10 - Average Validation Loss: 0.039325913442382635
2025-03-23 17:00:33,819 - INFO - Model and tokenizer saved to /storage/work/mqf5675/Masters/NLP/project/model_output/transformer_en_ita/model
2025-03-23 17:00:33,967 - INFO - Training history plot saved.
2025-03-23 17:00:33,973 - INFO - Computing BLEU score...
2025-03-23 17:42:57,629 - INFO - BLEU Score: 0.5665755563690393
2025-03-23 17:42:57,636 - INFO - Performing example translations...
2025-03-23 17:42:57,637 - INFO - Loading model from /storage/work/mqf5675/Masters/NLP/project/model_output/transformer_en_ita/model for translation...
2025-03-23 17:43:01,614 - INFO - Translated 'go.' to 'vai.'
2025-03-23 17:43:01,627 - INFO - Loading model from /storage/work/mqf5675/Masters/NLP/project/model_output/transformer_en_ita/model for translation...
2025-03-23 17:43:04,364 - INFO - Translated 'hi.' to 'ciao'
