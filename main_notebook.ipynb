{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88f7822a-a245-4c24-b722-e0a29b1ac39f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\15616\\anaconda3\\Lib\\site-packages\\torchvision\\io\\image.py:13: UserWarning: Failed to load image Python extension: '[WinError 127] The specified procedure could not be found'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n",
      "C:\\Users\\15616\\anaconda3\\Lib\\site-packages\\torchvision\\datapoints\\__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "C:\\Users\\15616\\anaconda3\\Lib\\site-packages\\torchvision\\transforms\\v2\\__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import os\n",
    "import subprocess\n",
    "from pydub import AudioSegment\n",
    "import shutil\n",
    "import uuid\n",
    "import re\n",
    "import whisper\n",
    "from googletrans import Translator\n",
    "from gtts import gTTS\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "import pickle  # Added import for pickle\n",
    "import time\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import edge_tts\n",
    "import numpy as np\n",
    "\n",
    "from elevenlabs import ElevenLabs  # Import ElevenLabs client\n",
    "\n",
    "from transformers import MarianTokenizer, MarianMTModel\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12c1ccdd-42e6-4bd9-a393-4c2f60ede096",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialize ElevenLabs client (replace with your API key)\n",
    "# elevenlabs_client = ElevenLabs(api_key=\"sk_a931f6a0acdf587ebe3a0b227eeb5890f192dc27711cd832\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d788b4b2-b01a-4e83-8284-83731b32c0b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "FEMALE_VOICES={'Vietnamese': 'vi-VN-HoaiMyNeural',\n",
    " 'Bengali': 'bn-BD-NabanitaNeural',\n",
    " 'Thai': 'th-TH-PremwadeeNeural',\n",
    " 'English': 'en-AU-NatashaNeural',\n",
    " 'Portuguese': 'pt-BR-FranciscaNeural',\n",
    " 'Arabic': 'ar-AE-FatimaNeural',\n",
    " 'Turkish': 'tr-TR-EmelNeural',\n",
    " 'Spanish': 'es-AR-ElenaNeural',\n",
    " 'Korean': 'ko-KR-SunHiNeural',\n",
    " 'French': 'fr-BE-CharlineNeural',\n",
    " 'Indonesian': 'id-ID-GadisNeural',\n",
    " 'Russian': 'ru-RU-SvetlanaNeural',\n",
    " 'Hindi': 'hi-IN-SwaraNeural',\n",
    " 'Japanese': 'ja-JP-NanamiNeural',\n",
    " 'Afrikaans': 'af-ZA-AdriNeural',\n",
    " 'Amharic': 'am-ET-MekdesNeural',\n",
    " 'Azerbaijani': 'az-AZ-BanuNeural',\n",
    " 'Bulgarian': 'bg-BG-KalinaNeural',\n",
    " 'Bosnian': 'bs-BA-VesnaNeural',\n",
    " 'Catalan': 'ca-ES-JoanaNeural',\n",
    " 'Czech': 'cs-CZ-VlastaNeural',\n",
    " 'Welsh': 'cy-GB-NiaNeural',\n",
    " 'Danish': 'da-DK-ChristelNeural',\n",
    " 'German': 'de-AT-IngridNeural',\n",
    " 'Greek': 'el-GR-AthinaNeural',\n",
    " 'Irish': 'ga-IE-OrlaNeural',\n",
    " 'Galician': 'gl-ES-SabelaNeural',\n",
    " 'Gujarati': 'gu-IN-DhwaniNeural',\n",
    " 'Hebrew': 'he-IL-HilaNeural',\n",
    " 'Croatian': 'hr-HR-GabrijelaNeural',\n",
    " 'Hungarian': 'hu-HU-NoemiNeural',\n",
    " 'Icelandic': 'is-IS-GudrunNeural',\n",
    " 'Italian': 'it-IT-ElsaNeural',\n",
    " 'Javanese': 'jv-ID-SitiNeural',\n",
    " 'Georgian': 'ka-GE-EkaNeural',\n",
    " 'Kazakh': 'kk-KZ-AigulNeural',\n",
    " 'Khmer': 'km-KH-SreymomNeural',\n",
    " 'Kannada': 'kn-IN-SapnaNeural',\n",
    " 'Lao': 'lo-LA-KeomanyNeural',\n",
    " 'Lithuanian': 'lt-LT-OnaNeural',\n",
    " 'Latvian': 'lv-LV-EveritaNeural',\n",
    " 'Macedonian': 'mk-MK-MarijaNeural',\n",
    " 'Malayalam': 'ml-IN-SobhanaNeural',\n",
    " 'Mongolian': 'mn-MN-YesuiNeural',\n",
    " 'Marathi': 'mr-IN-AarohiNeural',\n",
    " 'Malay': 'ms-MY-YasminNeural',\n",
    " 'Maltese': 'mt-MT-GraceNeural',\n",
    " 'Burmese': 'my-MM-NilarNeural',\n",
    " 'Norwegian Bokmål': 'nb-NO-PernilleNeural',\n",
    " 'Nepali': 'ne-NP-HemkalaNeural',\n",
    " 'Dutch': 'nl-BE-DenaNeural',\n",
    " 'Polish': 'pl-PL-ZofiaNeural',\n",
    " 'Pashto': 'ps-AF-LatifaNeural',\n",
    " 'Romanian': 'ro-RO-AlinaNeural',\n",
    " 'Sinhala': 'si-LK-ThiliniNeural',\n",
    " 'Slovak': 'sk-SK-ViktoriaNeural',\n",
    " 'Slovenian': 'sl-SI-PetraNeural',\n",
    " 'Somali': 'so-SO-UbaxNeural',\n",
    " 'Albanian': 'sq-AL-AnilaNeural',\n",
    " 'Serbian': 'sr-RS-SophieNeural',\n",
    " 'Sundanese': 'su-ID-TutiNeural',\n",
    " 'Swedish': 'sv-SE-SofieNeural',\n",
    " 'Swahili': 'sw-KE-ZuriNeural',\n",
    " 'Tamil': 'ta-IN-PallaviNeural',\n",
    " 'Telugu': 'te-IN-ShrutiNeural',\n",
    " 'Chinese': 'zh-CN-XiaoxiaoNeural',\n",
    " 'Ukrainian': 'uk-UA-PolinaNeural',\n",
    " 'Urdu': 'ur-IN-GulNeural',\n",
    " 'Uzbek': 'uz-UZ-MadinaNeural',\n",
    " 'Zulu': 'zu-ZA-ThandoNeural'}\n",
    "\n",
    "\n",
    "\n",
    "MALE_VOICES= {'Vietnamese': 'vi-VN-NamMinhNeural',\n",
    " 'Bengali': 'bn-BD-PradeepNeural',\n",
    " 'Thai': 'th-TH-NiwatNeural',\n",
    " 'English': 'en-AU-WilliamNeural',\n",
    " 'Portuguese': 'pt-BR-AntonioNeural',\n",
    " 'Arabic': 'ar-AE-HamdanNeural',\n",
    " 'Turkish': 'tr-TR-AhmetNeural',\n",
    " 'Spanish': 'es-AR-TomasNeural',\n",
    " 'Korean': 'ko-KR-HyunsuNeural',\n",
    " 'French': 'fr-BE-GerardNeural',\n",
    " 'Indonesian': 'id-ID-ArdiNeural',\n",
    " 'Russian': 'ru-RU-DmitryNeural',\n",
    " 'Hindi': 'hi-IN-MadhurNeural',\n",
    " 'Japanese': 'ja-JP-KeitaNeural',\n",
    " 'Afrikaans': 'af-ZA-WillemNeural',\n",
    " 'Amharic': 'am-ET-AmehaNeural',\n",
    " 'Azerbaijani': 'az-AZ-BabekNeural',\n",
    " 'Bulgarian': 'bg-BG-BorislavNeural',\n",
    " 'Bosnian': 'bs-BA-GoranNeural',\n",
    " 'Catalan': 'ca-ES-EnricNeural',\n",
    " 'Czech': 'cs-CZ-AntoninNeural',\n",
    " 'Welsh': 'cy-GB-AledNeural',\n",
    " 'Danish': 'da-DK-JeppeNeural',\n",
    " 'German': 'de-AT-JonasNeural',\n",
    " 'Greek': 'el-GR-NestorasNeural',\n",
    " 'Irish': 'ga-IE-ColmNeural',\n",
    " 'Galician': 'gl-ES-RoiNeural',\n",
    " 'Gujarati': 'gu-IN-NiranjanNeural',\n",
    " 'Hebrew': 'he-IL-AvriNeural',\n",
    " 'Croatian': 'hr-HR-SreckoNeural',\n",
    " 'Hungarian': 'hu-HU-TamasNeural',\n",
    " 'Icelandic': 'is-IS-GunnarNeural',\n",
    " 'Italian': 'it-IT-DiegoNeural',\n",
    " 'Javanese': 'jv-ID-DimasNeural',\n",
    " 'Georgian': 'ka-GE-GiorgiNeural',\n",
    " 'Kazakh': 'kk-KZ-DauletNeural',\n",
    " 'Khmer': 'km-KH-PisethNeural',\n",
    " 'Kannada': 'kn-IN-GaganNeural',\n",
    " 'Lao': 'lo-LA-ChanthavongNeural',\n",
    " 'Lithuanian': 'lt-LT-LeonasNeural',\n",
    " 'Latvian': 'lv-LV-NilsNeural',\n",
    " 'Macedonian': 'mk-MK-AleksandarNeural',\n",
    " 'Malayalam': 'ml-IN-MidhunNeural',\n",
    " 'Mongolian': 'mn-MN-BataaNeural',\n",
    " 'Marathi': 'mr-IN-ManoharNeural',\n",
    " 'Malay': 'ms-MY-OsmanNeural',\n",
    " 'Maltese': 'mt-MT-JosephNeural',\n",
    " 'Burmese': 'my-MM-ThihaNeural',\n",
    " 'Norwegian Bokmål': 'nb-NO-FinnNeural',\n",
    " 'Nepali': 'ne-NP-SagarNeural',\n",
    " 'Dutch': 'nl-BE-ArnaudNeural',\n",
    " 'Polish': 'pl-PL-MarekNeural',\n",
    " 'Pashto': 'ps-AF-GulNawazNeural',\n",
    " 'Romanian': 'ro-RO-EmilNeural',\n",
    " 'Sinhala': 'si-LK-SameeraNeural',\n",
    " 'Slovak': 'sk-SK-LukasNeural',\n",
    " 'Slovenian': 'sl-SI-RokNeural',\n",
    " 'Somali': 'so-SO-MuuseNeural',\n",
    " 'Albanian': 'sq-AL-IlirNeural',\n",
    " 'Serbian': 'sr-RS-NicholasNeural',\n",
    " 'Sundanese': 'su-ID-JajangNeural',\n",
    " 'Swedish': 'sv-SE-MattiasNeural',\n",
    " 'Swahili': 'sw-KE-RafikiNeural',\n",
    " 'Tamil': 'ta-IN-ValluvarNeural',\n",
    " 'Telugu': 'te-IN-MohanNeural',\n",
    " 'Chinese': 'zh-CN-YunjianNeural',\n",
    " 'Ukrainian': 'uk-UA-OstapNeural',\n",
    " 'Urdu': 'ur-IN-SalmanNeural',\n",
    " 'Uzbek': 'uz-UZ-SardorNeural',\n",
    " 'Zulu': 'zu-ZA-ThembaNeural'}\n",
    "\n",
    "\n",
    "LANGUAGES = {\n",
    "    \"Afrikaans\": \"af\",\n",
    "    \"Amharic\": \"am\",\n",
    "    \"Arabic\": \"ar\",\n",
    "    \"Azerbaijani\": \"az\",\n",
    "    \"Bulgarian\": \"bg\",\n",
    "    \"Bengali\": \"bn\",\n",
    "    \"Bosnian\": \"bs\",\n",
    "    \"Catalan\": \"ca\",\n",
    "    \"Czech\": \"cs\",\n",
    "    \"Welsh\": \"cy\",\n",
    "    \"Danish\": \"da\",\n",
    "    \"German\": \"de\",\n",
    "    \"Greek\": \"el\",\n",
    "    \"English\": \"en\",\n",
    "    \"Spanish\": \"es\",\n",
    "    \"French\": \"fr\",\n",
    "    \"Irish\": \"ga\",\n",
    "    \"Galician\": \"gl\",\n",
    "    \"Gujarati\": \"gu\",\n",
    "    \"Hebrew\": \"he\",\n",
    "    \"Hindi\": \"hi\",\n",
    "    \"Croatian\": \"hr\",\n",
    "    \"Hungarian\": \"hu\",\n",
    "    \"Indonesian\": \"id\",\n",
    "    \"Icelandic\": \"is\",\n",
    "    \"Italian\": \"it\",\n",
    "    \"Japanese\": \"ja\",\n",
    "    \"Javanese\": \"jv\",\n",
    "    \"Georgian\": \"ka\",\n",
    "    \"Kazakh\": \"kk\",\n",
    "    \"Khmer\": \"km\",\n",
    "    \"Kannada\": \"kn\",\n",
    "    \"Korean\": \"ko\",\n",
    "    \"Lao\": \"lo\",\n",
    "    \"Lithuanian\": \"lt\",\n",
    "    \"Latvian\": \"lv\",\n",
    "    \"Macedonian\": \"mk\",\n",
    "    \"Malayalam\": \"ml\",\n",
    "    \"Mongolian\": \"mn\",\n",
    "    \"Marathi\": \"mr\",\n",
    "    \"Malay\": \"ms\",\n",
    "    \"Maltese\": \"mt\",\n",
    "    \"Burmese\": \"my\",\n",
    "    \"Norwegian Bokmål\": \"nb\",\n",
    "    \"Nepali\": \"ne\",\n",
    "    \"Dutch\": \"nl\",\n",
    "    \"Polish\": \"pl\",\n",
    "    \"Pashto\": \"ps\",\n",
    "    \"Portuguese\": \"pt\",\n",
    "    \"Romanian\": \"ro\",\n",
    "    \"Russian\": \"ru\",\n",
    "    \"Sinhala\": \"si\",\n",
    "    \"Slovak\": \"sk\",\n",
    "    \"Slovenian\": \"sl\",\n",
    "    \"Somali\": \"so\",\n",
    "    \"Albanian\": \"sq\",\n",
    "    \"Serbian\": \"sr\",\n",
    "    \"Sundanese\": \"su\",\n",
    "    \"Swedish\": \"sv\",\n",
    "    \"Swahili\": \"sw\",\n",
    "    \"Tamil\": \"ta\",\n",
    "    \"Telugu\": \"te\",\n",
    "    \"Thai\": \"th\",\n",
    "    \"Turkish\": \"tr\",\n",
    "    \"Ukrainian\": \"uk\",\n",
    "    \"Urdu\": \"ur\",\n",
    "    \"Uzbek\": \"uz\",\n",
    "    \"Vietnamese\": \"vi\",\n",
    "    \"Chinese\": \"zh\",\n",
    "    \"Zulu\": \"zu\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "478bbe79-26c9-49c2-b1da-535a16c881c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AudioTranscriber:\n",
    "    \"\"\"Handles audio transcription using Whisper.\"\"\"\n",
    "    def __init__(self, model_name=\"base\"):\n",
    "        self.model = whisper.load_model(model_name)\n",
    "\n",
    "    def transcribe(self, audio_filepath, output_txt_path, output_srt_path):\n",
    "        if not os.path.exists(audio_filepath):\n",
    "            raise FileNotFoundError(f\"Audio file not found: {audio_filepath}\")\n",
    "        \n",
    "        result = self.model.transcribe(audio_filepath, word_timestamps=True, fp16=False)\n",
    "        \n",
    "        os.makedirs(os.path.dirname(output_txt_path), exist_ok=True)\n",
    "        with open(output_txt_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(result[\"text\"])\n",
    "        \n",
    "        os.makedirs(os.path.dirname(output_srt_path), exist_ok=True)\n",
    "        with open(output_srt_path, 'w', encoding='utf-8') as f:\n",
    "            for i, segment in enumerate(result[\"segments\"], 1):\n",
    "                start = segment[\"start\"]\n",
    "                end = segment[\"end\"]\n",
    "                text = segment[\"text\"].strip()\n",
    "                f.write(f\"{i}\\n\")\n",
    "                f.write(f\"{self._format_srt_timestamp(start)} --> {self._format_srt_timestamp(end)}\\n\")\n",
    "                f.write(f\"{text}\\n\\n\")\n",
    "        \n",
    "        print(f\"Transcription saved at: {output_txt_path}\")\n",
    "        print(f\"Original SRT saved at: {output_srt_path}\")\n",
    "        return output_srt_path, result[\"text\"]\n",
    "\n",
    "    @staticmethod\n",
    "    def _format_srt_timestamp(seconds):\n",
    "        hours = int(seconds // 3600)\n",
    "        minutes = int((seconds % 3600) // 60)\n",
    "        secs = int(seconds % 60)\n",
    "        millis = int((seconds % 1) * 1000)\n",
    "        return f\"{hours:02d}:{minutes:02d}:{secs:02d},{millis:03d}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a908fe-aec8-420e-8c93-e9426c730be9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5ce954fb-2239-49b4-98f2-cc361d7e3d02",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TextToSpeech:\n",
    "    \"\"\"Handles text-to-speech conversion with multi-language support.\"\"\"\n",
    "    def __init__(self, speed=1):\n",
    "        self.speed = speed\n",
    "\n",
    "    async def convert(self, text, output_path, language, gender, should_translate=False):\n",
    "        target_language = LANGUAGES.get(language, \"en\")\n",
    "        input_text = await self._translate_text(text, language) if should_translate else text\n",
    "        print(f\"Translated to {language}: {input_text}\" if should_translate else f\"Processing: {input_text}\")\n",
    "        \n",
    "        os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "        \n",
    "        # Choose voice based on gender\n",
    "        voice = FEMALE_VOICES.get(language, \"en-AU-NatashaNeural\") if gender == \"Female\" else MALE_VOICES.get(language, \"en-AU-WilliamNeural\")\n",
    "        \n",
    "        # Use edge-tts for audio generation\n",
    "        tts = edge_tts.Communicate(text=input_text, voice=voice)\n",
    "        await tts.save(output_path)\n",
    "        \n",
    "        audio = AudioSegment.from_file(output_path)\n",
    "        if self.speed != 1:\n",
    "            audio = self._adjust_speed(audio, self.speed)\n",
    "        audio.export(output_path, format=\"mp3\")\n",
    "        print(f\"Generated audio: {output_path}, Duration: {audio.duration_seconds}s\")\n",
    "        return audio\n",
    "\n",
    "    async def _translate_text(self, text, target_language):\n",
    "        \"\"\"Translate text based on the target language using appropriate model.\"\"\"\n",
    "        if target_language == \"Turkish\":\n",
    "            return await self._loadTurkishModelAndTranslate(text)\n",
    "        elif target_language in [\"Italian\", \"Spanish\", \"Dutch\"]:\n",
    "            model_paths = {\n",
    "                \"Italian\": \"transformer_en_ita_model\",\n",
    "                \"Spanish\": \"transformer_en_es_model\",\n",
    "                \"Dutch\": \"transformer_en_deu_model\"\n",
    "            }\n",
    "            return self._load_and_translate(model_paths[target_language], text)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported language for translation: {target_language}\")\n",
    "\n",
    "    async def _loadTurkishModelAndTranslate(self, test_sentence):\n",
    "        \"\"\"Load Turkish Seq2Seq model and translate the input sentence.\"\"\"\n",
    "        output_dir = 'seq2seq'\n",
    "        with open(f'{output_dir}/tokenizer/input_tokenizer.pkl', 'rb') as f:\n",
    "            loaded_input_tokenizer = pickle.load(f)\n",
    "        with open(f'{output_dir}/tokenizer/target_tokenizer.pkl', 'rb') as f:\n",
    "            loaded_target_tokenizer = pickle.load(f)\n",
    "        with open(f'{output_dir}/tokenizer/maxlen_values.pkl', 'rb') as f:\n",
    "            maxlen = pickle.load(f)\n",
    "        \n",
    "        loaded_encoder_model = load_model(output_dir + '/encoder_model.h5', compile=False)\n",
    "        loaded_decoder_model = load_model(output_dir + '/decoder_model.h5', compile=False)\n",
    "        \n",
    "        translation = self.translate_sentence(test_sentence, loaded_encoder_model, loaded_decoder_model, \n",
    "                                             loaded_input_tokenizer, loaded_target_tokenizer, \n",
    "                                             maxlen['input_maxlen'], maxlen['target_maxlen'])\n",
    "        print(f\"Input Sentence: {test_sentence}\")\n",
    "        print(f\"Translated Sentence: {translation}\")\n",
    "        return translation\n",
    "\n",
    "    def translate_sentence(self, sentence, encoder_model, decoder_model, input_tokenizer, target_tokenizer, input_maxlen, target_maxlen):\n",
    "        \"\"\"Translate a sentence using the loaded Seq2Seq model.\"\"\"\n",
    "        input_seq = input_tokenizer.texts_to_sequences([sentence])\n",
    "        input_seq = pad_sequences(input_seq, maxlen=input_maxlen, padding='post')\n",
    "        states_value = encoder_model.predict(input_seq, verbose=0)\n",
    "        \n",
    "        target_seq = np.zeros((1, 1))\n",
    "        target_seq[0, 0] = target_tokenizer.word_index['<start>']\n",
    "        stop_condition = False\n",
    "        decoded_sentence = ''\n",
    "        end_token_idx = target_tokenizer.word_index['<end>']\n",
    "\n",
    "        while not stop_condition:\n",
    "            output_tokens, h, c = decoder_model.predict([target_seq] + states_value, verbose=0)\n",
    "            sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "            sampled_word = target_tokenizer.index_word.get(sampled_token_index, '')\n",
    "            if sampled_word == '<end>' or len(decoded_sentence.split()) >= target_maxlen:\n",
    "                stop_condition = True\n",
    "            elif sampled_word != '<start>':\n",
    "                decoded_sentence += sampled_word + ' '\n",
    "            target_seq = np.zeros((1, 1))\n",
    "            target_seq[0, 0] = sampled_token_index\n",
    "            states_value = [h, c]\n",
    "        \n",
    "        return decoded_sentence.strip()\n",
    "\n",
    "    def _load_and_translate(self, model_path, text, max_length=128, device=None):\n",
    "        \"\"\"Load MarianMT model and translate text.\"\"\"\n",
    "        if device is None:\n",
    "            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        print(f\"Loading MarianMT model from {model_path} for translation...\")\n",
    "        tokenizer = MarianTokenizer.from_pretrained(model_path)\n",
    "        model = MarianMTModel.from_pretrained(model_path)\n",
    "        model.to(device)\n",
    "        model.eval()\n",
    "        \n",
    "        print(f\"Tokenizing input text: '{text}'\")\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=max_length).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            translated = model.generate(**inputs)\n",
    "        \n",
    "        translation = tokenizer.decode(translated[0], skip_special_tokens=True)\n",
    "        print(f\"Translated '{text}' to '{translation}'\")\n",
    "        return translation\n",
    "\n",
    "    def _adjust_speed(self, audio, speed):\n",
    "        \"\"\"Adjust playback speed using pydub.\"\"\"\n",
    "        new_frame_rate = int(audio.frame_rate * speed)\n",
    "        return audio._spawn(audio.raw_data, overrides={'frame_rate': new_frame_rate}).set_frame_rate(audio.frame_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1eba739b-a048-45e5-8a15-bcbdfe7b51d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SubtitleDubber:\n",
    "    \"\"\"Handles subtitle dubbing and SRT translation.\"\"\"\n",
    "    def __init__(self, transcriber: AudioTranscriber, tts: TextToSpeech):\n",
    "        self.transcriber = transcriber\n",
    "        self.tts = tts\n",
    "\n",
    "    async def dub_srt(self, srt_file_path, dub_save_path, language=\"Japanese\", gender=\"Male\"):\n",
    "        \"\"\"Dub SRT file into audio and generate translated SRT.\"\"\"\n",
    "        subtitle_data = self._parse_srt_file(srt_file_path)\n",
    "        new_folder_path = self._create_directory_for_srt(srt_file_path)\n",
    "        audio_files_to_merge = []\n",
    "\n",
    "        for subtitle in subtitle_data:\n",
    "            text = subtitle['text']\n",
    "            actual_duration = subtitle['end_time'] - subtitle['start_time']\n",
    "            pause_time = subtitle['pause_time']\n",
    "            silence_path = f\"{new_folder_path}/{subtitle['previous_pause']}\"\n",
    "            self._create_silence(pause_time, silence_path)\n",
    "            audio_files_to_merge.append(silence_path)\n",
    "            audio_path = f\"{new_folder_path}/{subtitle['audio_name']}\"\n",
    "            await self._convert_text_to_speech(text, audio_path, language, gender, actual_duration)\n",
    "            audio_files_to_merge.append(audio_path)\n",
    "\n",
    "        await self._merge_audio_files(audio_files_to_merge, dub_save_path)\n",
    "        translated_srt_path = os.path.splitext(dub_save_path)[0] + \"_translated.srt\"\n",
    "        await self._generate_translated_srt(subtitle_data, language, translated_srt_path)\n",
    "\n",
    "    async def _convert_text_to_speech(self, text, audio_output_path, language, gender, actual_duration):\n",
    "        temp_filename = \"D:/Work/Faculty/Masters/NLP/project/content/temp_audio1.mp3\"\n",
    "        os.makedirs(os.path.dirname(temp_filename), exist_ok=True)\n",
    "        audio = await self.tts.convert(text, temp_filename, language, gender, should_translate=True)\n",
    "        \n",
    "        if audio is None:\n",
    "            print(f\"Skipping {text} due to invalid or missing audio file: {temp_filename}\")\n",
    "            silence = AudioSegment.silent(duration=actual_duration)\n",
    "            silence.export(audio_output_path, format=\"mp3\")\n",
    "            return\n",
    "\n",
    "        tts_duration = len(audio)\n",
    "        print(f\"Generated temp audio duration: {tts_duration/1000}s, Expected: {actual_duration/1000}s\")\n",
    "\n",
    "        if actual_duration == 0:\n",
    "            shutil.move(temp_filename, audio_output_path)\n",
    "            return\n",
    "\n",
    "        if tts_duration > actual_duration:\n",
    "            speedup_factor = tts_duration / actual_duration\n",
    "            speedup_filename = \"D:/Work/Faculty/Masters/NLP/project/content/sped_up_audio.mp3\"\n",
    "            subprocess.run([\n",
    "                \"ffmpeg\", \"-i\", temp_filename, \"-filter:a\", f\"atempo={speedup_factor}\", \"-f\", \"mp3\", speedup_filename\n",
    "            ], check=True)\n",
    "            shutil.move(speedup_filename, audio_output_path)\n",
    "        elif tts_duration < actual_duration:\n",
    "            silence_gap = actual_duration - tts_duration\n",
    "            silence = AudioSegment.silent(duration=int(silence_gap))\n",
    "            new_audio = audio + silence\n",
    "            new_audio.export(audio_output_path, format=\"mp3\")\n",
    "        else:\n",
    "            shutil.move(temp_filename, audio_output_path)\n",
    "\n",
    "    @staticmethod\n",
    "    def _create_silence(pause_duration, silence_file_path):\n",
    "        min_duration = max(pause_duration, 100)\n",
    "        silence = AudioSegment.silent(duration=min_duration)\n",
    "        silence.export(silence_file_path, format=\"mp3\")\n",
    "        audio = AudioSegment.from_file(silence_file_path)\n",
    "        print(f\"Created silence file: {silence_file_path}, Duration: {audio.duration_seconds}s\")\n",
    "        return silence_file_path\n",
    "\n",
    "    @staticmethod\n",
    "    def _create_directory_for_srt(srt_file_path):\n",
    "        srt_base_name = os.path.splitext(os.path.basename(srt_file_path))[0]\n",
    "        random_uuid = str(uuid.uuid4())[:4]\n",
    "        base_directory = \"D:/Work/Faculty/Masters/NLP/project/content/dummy\"\n",
    "        os.makedirs(base_directory, exist_ok=True)\n",
    "        new_directory = os.path.join(base_directory, f\"{srt_base_name}_{random_uuid}\")\n",
    "        os.makedirs(new_directory, exist_ok=True)\n",
    "        return new_directory\n",
    "\n",
    "    @staticmethod\n",
    "    async def _merge_audio_files(audio_paths, output_path):\n",
    "        merged_audio = AudioSegment.silent(duration=0)\n",
    "        for audio_path in audio_paths:\n",
    "            print(f\"Merging: {audio_path}\")\n",
    "            audio = AudioSegment.from_file(audio_path)\n",
    "            merged_audio += audio\n",
    "        merged_audio.export(output_path, format=\"mp3\")\n",
    "\n",
    "    async def _generate_translated_srt(self, subtitle_data, language, srt_output_path):\n",
    "        translated_entries = []\n",
    "        for entry in subtitle_data:\n",
    "            # translated_text = await self.tts._translate(entry['text'], language)  # Use tts._translate()\n",
    "            translated_text = await self.tts._loadTurkishModelAndTranslate(entry['text'])\n",
    "            translated_entries.append({\n",
    "                'entry_number': entry['entry_number'],\n",
    "                'start_time': entry['start_time'],\n",
    "                'end_time': entry['end_time'],\n",
    "                'text': translated_text\n",
    "            })\n",
    "        \n",
    "        with open(srt_output_path, 'w', encoding='utf-8') as f:\n",
    "            for entry in translated_entries:\n",
    "                f.write(f\"{entry['entry_number']}\\n\")\n",
    "                f.write(f\"{self._format_srt_timestamp(entry['start_time'])} --> {self._format_srt_timestamp(entry['end_time'])}\\n\")\n",
    "                f.write(f\"{entry['text']}\\n\\n\")\n",
    "        print(f\"Translated SRT saved at: {srt_output_path}\")\n",
    "\n",
    "    @staticmethod\n",
    "    def _parse_srt_file(file_path):\n",
    "        subtitle_entries = []\n",
    "        default_start_time = 0\n",
    "        previous_end_time = default_start_time\n",
    "        entry_count = 1\n",
    "        audio_name_format = \"{}.mp3\"\n",
    "        pause_name_format = \"{}_before_pause.mp3\"\n",
    "\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            lines = file.readlines()\n",
    "            for i in range(0, len(lines), 4):\n",
    "                time_info = re.findall(r'(\\d+:\\d+:\\d+,\\d+) --> (\\d+:\\d+:\\d+,\\d+)', lines[i + 1])\n",
    "                start_time = SubtitleDubber._convert_to_milliseconds(time_info[0][0])\n",
    "                end_time = SubtitleDubber._convert_to_milliseconds(time_info[0][1])\n",
    "\n",
    "                current_entry = {\n",
    "                    'entry_number': entry_count,\n",
    "                    'start_time': start_time,\n",
    "                    'end_time': end_time,\n",
    "                    'text': lines[i + 2].strip(),\n",
    "                    'pause_time': start_time - previous_end_time if entry_count != 1 else start_time - default_start_time,\n",
    "                    'audio_name': audio_name_format.format(entry_count),\n",
    "                    'previous_pause': pause_name_format.format(entry_count),\n",
    "                }\n",
    "\n",
    "                subtitle_entries.append(current_entry)\n",
    "                previous_end_time = end_time\n",
    "                entry_count += 1\n",
    "\n",
    "        return subtitle_entries\n",
    "\n",
    "    @staticmethod\n",
    "    def _convert_to_milliseconds(time_str):\n",
    "        hours, minutes, second_millisecond = time_str.split(':')\n",
    "        seconds, milliseconds = second_millisecond.split(\",\")\n",
    "        return (\n",
    "            int(hours) * 3600000 +\n",
    "            int(minutes) * 60000 +\n",
    "            int(seconds) * 1000 +\n",
    "            int(milliseconds)\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def _format_srt_timestamp(milliseconds):\n",
    "        seconds = milliseconds / 1000\n",
    "        hours = int(seconds // 3600)\n",
    "        minutes = int((seconds % 3600) // 60)\n",
    "        secs = int(seconds % 60)\n",
    "        millis = int((seconds % 1) * 1000)\n",
    "        return f\"{hours:02d}:{minutes:02d}:{secs:02d},{millis:03d}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a9cbe1-bdd0-466d-aeb4-eabd585f6640",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "398a4a7f-62a1-4620-9e0f-1b68059c0a73",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class VideoDubber:\n",
    "    \"\"\"Handles video dubbing with FFmpeg.\"\"\"\n",
    "    @staticmethod\n",
    "    async def dub_video(original_video_path, dubbed_audio_path, output_video_path):\n",
    "        \"\"\"Replace original audio with dubbed audio in the video.\"\"\"\n",
    "        os.makedirs(os.path.dirname(output_video_path), exist_ok=True)\n",
    "        ffmpeg_command = [\n",
    "            \"ffmpeg\",\n",
    "            \"-i\", original_video_path,\n",
    "            \"-i\", dubbed_audio_path,\n",
    "            \"-c:v\", \"copy\",\n",
    "            \"-map\", \"0:v:0\",\n",
    "            \"-map\", \"1:a:0\",\n",
    "            \"-shortest\",\n",
    "            \"-y\",\n",
    "            output_video_path\n",
    "        ]\n",
    "        result = subprocess.run(ffmpeg_command, capture_output=True, text=True)\n",
    "        if result.returncode == 0:\n",
    "            print(f\"Dubbed video saved at: {output_video_path}\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"Failed to generate video: {result.stderr}\")\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5342b40-15e7-4430-8ec3-34322a7ba4c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9493b3d4-489d-4e90-9f54-b09a7585b671",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription saved at: D:/Work/Faculty/Masters/NLP/project/content/transcription_output.txt\n",
      "Original SRT saved at: D:/Work/Faculty/Masters/NLP/project/content/original_subtitles.srt\n",
      "Created silence file: D:/Work/Faculty/Masters/NLP/project/content/dummy\\original_subtitles_2ce5/1_before_pause.mp3, Duration: 0.09995464852607709s\n",
      "Loading MarianMT model from transformer_en_deu_model for translation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\15616\\anaconda3\\Lib\\site-packages\\transformers\\models\\marian\\tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing input text: 'But while we are fighting every day to build up our nation,'\n",
      "Translated 'But while we are fighting every day to build up our nation,' to 'aber während wir jeden tag um den bau unserer nation kämpfen'\n",
      "Translated to Dutch: aber während wir jeden tag um den bau unserer nation kämpfen\n",
      "Generated audio: D:/Work/Faculty/Masters/NLP/project/content/temp_audio1.mp3, Duration: 4.416s\n",
      "Generated temp audio duration: 4.416s, Expected: 4.059s\n",
      "Created silence file: D:/Work/Faculty/Masters/NLP/project/content/dummy\\original_subtitles_2ce5/2_before_pause.mp3, Duration: 0.22095238095238096s\n",
      "Loading MarianMT model from transformer_en_deu_model for translation...\n",
      "Tokenizing input text: 'the far left only wants to wreck ruin and destroy our nation.'\n",
      "Translated 'the far left only wants to wreck ruin and destroy our nation.' to 'die übrige Seite will nur noch ruinieren und unsere nation zerstören.'\n",
      "Translated to Dutch: die übrige Seite will nur noch ruinieren und unsere nation zerstören.\n",
      "Generated audio: D:/Work/Faculty/Masters/NLP/project/content/temp_audio1.mp3, Duration: 4.752s\n",
      "Generated temp audio duration: 4.752s, Expected: 4.059s\n",
      "Created silence file: D:/Work/Faculty/Masters/NLP/project/content/dummy\\original_subtitles_2ce5/3_before_pause.mp3, Duration: 2.220952380952381s\n",
      "Loading MarianMT model from transformer_en_deu_model for translation...\n",
      "Tokenizing input text: 'And you know better than anybody.'\n",
      "Translated 'And you know better than anybody.' to 'Und du weißt es besser als jeder sonst.'\n",
      "Translated to Dutch: Und du weißt es besser als jeder sonst.\n",
      "Generated audio: D:/Work/Faculty/Masters/NLP/project/content/temp_audio1.mp3, Duration: 3.264s\n",
      "Generated temp audio duration: 3.264s, Expected: 2.059s\n",
      "Created silence file: D:/Work/Faculty/Masters/NLP/project/content/dummy\\original_subtitles_2ce5/4_before_pause.mp3, Duration: 0.26095238095238094s\n",
      "Loading MarianMT model from transformer_en_deu_model for translation...\n",
      "Tokenizing input text: 'For the last three years, Democrat lawmakers,'\n",
      "Translated 'For the last three years, Democrat lawmakers,' to 'in den letzten drei jahren haben wir demokratische gesetzgeber'\n",
      "Translated to Dutch: in den letzten drei jahren haben wir demokratische gesetzgeber\n",
      "Generated audio: D:/Work/Faculty/Masters/NLP/project/content/temp_audio1.mp3, Duration: 4.176s\n",
      "Generated temp audio duration: 4.176s, Expected: 2.84s\n",
      "Created silence file: D:/Work/Faculty/Masters/NLP/project/content/dummy\\original_subtitles_2ce5/5_before_pause.mp3, Duration: 0.9989115646258503s\n",
      "Loading MarianMT model from transformer_en_deu_model for translation...\n",
      "Tokenizing input text: 'their deep state cronies, the fake news media,'\n",
      "Translated 'their deep state cronies, the fake news media,' to 'ihre tief im staat lebenden cronies die fälschungen der nachrichtenträger'\n",
      "Translated to Dutch: ihre tief im staat lebenden cronies die fälschungen der nachrichtenträger\n",
      "Generated audio: D:/Work/Faculty/Masters/NLP/project/content/temp_audio1.mp3, Duration: 4.8s\n",
      "Generated temp audio duration: 4.8s, Expected: 4.201s\n",
      "Created silence file: D:/Work/Faculty/Masters/NLP/project/content/dummy\\original_subtitles_2ce5/6_before_pause.mp3, Duration: 0.09995464852607709s\n",
      "Loading MarianMT model from transformer_en_deu_model for translation...\n",
      "Tokenizing input text: 'they've been colluding in their effort to overturn'\n",
      "Translated 'they've been colluding in their effort to overturn' to 'sie haben sich in ihrem versuch zusammengetan umzukehren'\n",
      "Translated to Dutch: sie haben sich in ihrem versuch zusammengetan umzukehren\n",
      "Generated audio: D:/Work/Faculty/Masters/NLP/project/content/temp_audio1.mp3, Duration: 4.2s\n",
      "Generated temp audio duration: 4.2s, Expected: 3.4s\n",
      "Created silence file: D:/Work/Faculty/Masters/NLP/project/content/dummy\\original_subtitles_2ce5/7_before_pause.mp3, Duration: 0.09995464852607709s\n",
      "Loading MarianMT model from transformer_en_deu_model for translation...\n",
      "Tokenizing input text: 'the presidential election.'\n",
      "Translated 'the presidential election.' to 'die präsidentialwahl.'\n",
      "Translated to Dutch: die präsidentialwahl.\n",
      "Generated audio: D:/Work/Faculty/Masters/NLP/project/content/temp_audio1.mp3, Duration: 2.232s\n",
      "Generated temp audio duration: 2.232s, Expected: 1.559s\n",
      "Created silence file: D:/Work/Faculty/Masters/NLP/project/content/dummy\\original_subtitles_2ce5/8_before_pause.mp3, Duration: 0.7609977324263039s\n",
      "Loading MarianMT model from transformer_en_deu_model for translation...\n",
      "Tokenizing input text: '63 million people voted and to nullify the votes of the American people.'\n",
      "Translated '63 million people voted and to nullify the votes of the American people.' to '63 millionen leute haben gewählt und die stimmen des amerikanischen volks aufgehoben.'\n",
      "Translated to Dutch: 63 millionen leute haben gewählt und die stimmen des amerikanischen volks aufgehoben.\n",
      "Generated audio: D:/Work/Faculty/Masters/NLP/project/content/temp_audio1.mp3, Duration: 6.12s\n",
      "Generated temp audio duration: 6.12s, Expected: 6.039s\n",
      "Created silence file: D:/Work/Faculty/Masters/NLP/project/content/dummy\\original_subtitles_2ce5/9_before_pause.mp3, Duration: 0.46095238095238095s\n",
      "Loading MarianMT model from transformer_en_deu_model for translation...\n",
      "Tokenizing input text: 'And many African-American people voted for Trump.'\n",
      "Translated 'And many African-American people voted for Trump.' to 'Und viele afroamerikaner wählten Trump.'\n",
      "Translated to Dutch: Und viele afroamerikaner wählten Trump.\n",
      "Generated audio: D:/Work/Faculty/Masters/NLP/project/content/temp_audio1.mp3, Duration: 3.24s\n",
      "Generated temp audio duration: 3.24s, Expected: 2.899s\n",
      "Created silence file: D:/Work/Faculty/Masters/NLP/project/content/dummy\\original_subtitles_2ce5/10_before_pause.mp3, Duration: 0.7409523809523809s\n",
      "Loading MarianMT model from transformer_en_deu_model for translation...\n",
      "Tokenizing input text: 'Even then, now they like me more because I said I was going to do it,'\n",
      "Translated 'Even then, now they like me more because I said I was going to do it,' to 'selbst damals mögen sie mich jetzt mehr weil ich gesagt hatte dass ich es tun würde.'\n",
      "Translated to Dutch: selbst damals mögen sie mich jetzt mehr weil ich gesagt hatte dass ich es tun würde.\n",
      "Generated audio: D:/Work/Faculty/Masters/NLP/project/content/temp_audio1.mp3, Duration: 5.4s\n",
      "Generated temp audio duration: 5.4s, Expected: 2.979s\n",
      "Created silence file: D:/Work/Faculty/Masters/NLP/project/content/dummy\\original_subtitles_2ce5/11_before_pause.mp3, Duration: 0.09995464852607709s\n",
      "Loading MarianMT model from transformer_en_deu_model for translation...\n",
      "Tokenizing input text: 'and now I did it, so you better vote.'\n",
      "Translated 'and now I did it, so you better vote.' to 'und jetzt habe ich es getan. du solltest also lieber stimmen.'\n",
      "Translated to Dutch: und jetzt habe ich es getan. du solltest also lieber stimmen.\n",
      "Generated audio: D:/Work/Faculty/Masters/NLP/project/content/temp_audio1.mp3, Duration: 5.376s\n",
      "Generated temp audio duration: 5.376s, Expected: 1.5s\n",
      "Created silence file: D:/Work/Faculty/Masters/NLP/project/content/dummy\\original_subtitles_2ce5/12_before_pause.mp3, Duration: 2.1609977324263037s\n",
      "Loading MarianMT model from transformer_en_deu_model for translation...\n",
      "Tokenizing input text: 'I said I was going to do it.'\n",
      "Translated 'I said I was going to do it.' to 'ich sagte dass ich es tun würde.'\n",
      "Translated to Dutch: ich sagte dass ich es tun würde.\n",
      "Generated audio: D:/Work/Faculty/Masters/NLP/project/content/temp_audio1.mp3, Duration: 2.712s\n",
      "Generated temp audio duration: 2.712s, Expected: 1.379s\n",
      "Created silence file: D:/Work/Faculty/Masters/NLP/project/content/dummy\\original_subtitles_2ce5/13_before_pause.mp3, Duration: 1.5809523809523809s\n",
      "Loading MarianMT model from transformer_en_deu_model for translation...\n",
      "Tokenizing input text: 'I said I'm going to do it.'\n",
      "Translated 'I said I'm going to do it.' to 'ich sagte dass ich es tun werde.'\n",
      "Translated to Dutch: ich sagte dass ich es tun werde.\n",
      "Generated audio: D:/Work/Faculty/Masters/NLP/project/content/temp_audio1.mp3, Duration: 2.664s\n",
      "Generated temp audio duration: 2.664s, Expected: 1.36s\n",
      "Created silence file: D:/Work/Faculty/Masters/NLP/project/content/dummy\\original_subtitles_2ce5/14_before_pause.mp3, Duration: 0.6989569160997733s\n",
      "Loading MarianMT model from transformer_en_deu_model for translation...\n",
      "Tokenizing input text: 'Yeah?'\n",
      "Translated 'Yeah?' to 'Und was ist das für ein spaß'\n",
      "Translated to Dutch: Und was ist das für ein spaß\n",
      "Generated audio: D:/Work/Faculty/Masters/NLP/project/content/temp_audio1.mp3, Duration: 2.808s\n",
      "Generated temp audio duration: 2.808s, Expected: 0.64s\n",
      "Created silence file: D:/Work/Faculty/Masters/NLP/project/content/dummy\\original_subtitles_2ce5/15_before_pause.mp3, Duration: 0.30095238095238097s\n",
      "Loading MarianMT model from transformer_en_deu_model for translation...\n",
      "Tokenizing input text: 'Long live, long live, long live, long live, long live.'\n",
      "Translated 'Long live, long live, long live, long live, long live.' to 'Es lebe es lebe es lebe es lebe es lebe es lebe es lebe es lebe es lebe es lebe es lebe es lebe es lebe es'\n",
      "Translated to Dutch: Es lebe es lebe es lebe es lebe es lebe es lebe es lebe es lebe es lebe es lebe es lebe es lebe es lebe es\n",
      "Generated audio: D:/Work/Faculty/Masters/NLP/project/content/temp_audio1.mp3, Duration: 8.112s\n",
      "Generated temp audio duration: 8.112s, Expected: 4.059s\n",
      "Created silence file: D:/Work/Faculty/Masters/NLP/project/content/dummy\\original_subtitles_2ce5/16_before_pause.mp3, Duration: 0.13995464852607709s\n",
      "Loading MarianMT model from transformer_en_deu_model for translation...\n",
      "Tokenizing input text: 'Thank you, folks.'\n",
      "Translated 'Thank you, folks.' to 'danke sehr.'\n",
      "Translated to Dutch: danke sehr.\n",
      "Generated audio: D:/Work/Faculty/Masters/NLP/project/content/temp_audio1.mp3, Duration: 1.824s\n",
      "Generated temp audio duration: 1.824s, Expected: 0.58s\n",
      "Merging: D:/Work/Faculty/Masters/NLP/project/content/dummy\\original_subtitles_2ce5/1_before_pause.mp3\n",
      "Merging: D:/Work/Faculty/Masters/NLP/project/content/dummy\\original_subtitles_2ce5/1.mp3\n",
      "Merging: D:/Work/Faculty/Masters/NLP/project/content/dummy\\original_subtitles_2ce5/2_before_pause.mp3\n",
      "Merging: D:/Work/Faculty/Masters/NLP/project/content/dummy\\original_subtitles_2ce5/2.mp3\n",
      "Merging: D:/Work/Faculty/Masters/NLP/project/content/dummy\\original_subtitles_2ce5/3_before_pause.mp3\n",
      "Merging: D:/Work/Faculty/Masters/NLP/project/content/dummy\\original_subtitles_2ce5/3.mp3\n",
      "Merging: D:/Work/Faculty/Masters/NLP/project/content/dummy\\original_subtitles_2ce5/4_before_pause.mp3\n",
      "Merging: D:/Work/Faculty/Masters/NLP/project/content/dummy\\original_subtitles_2ce5/4.mp3\n",
      "Merging: D:/Work/Faculty/Masters/NLP/project/content/dummy\\original_subtitles_2ce5/5_before_pause.mp3\n",
      "Merging: D:/Work/Faculty/Masters/NLP/project/content/dummy\\original_subtitles_2ce5/5.mp3\n",
      "Merging: D:/Work/Faculty/Masters/NLP/project/content/dummy\\original_subtitles_2ce5/6_before_pause.mp3\n",
      "Merging: D:/Work/Faculty/Masters/NLP/project/content/dummy\\original_subtitles_2ce5/6.mp3\n",
      "Merging: D:/Work/Faculty/Masters/NLP/project/content/dummy\\original_subtitles_2ce5/7_before_pause.mp3\n",
      "Merging: D:/Work/Faculty/Masters/NLP/project/content/dummy\\original_subtitles_2ce5/7.mp3\n",
      "Merging: D:/Work/Faculty/Masters/NLP/project/content/dummy\\original_subtitles_2ce5/8_before_pause.mp3\n",
      "Merging: D:/Work/Faculty/Masters/NLP/project/content/dummy\\original_subtitles_2ce5/8.mp3\n",
      "Merging: D:/Work/Faculty/Masters/NLP/project/content/dummy\\original_subtitles_2ce5/9_before_pause.mp3\n",
      "Merging: D:/Work/Faculty/Masters/NLP/project/content/dummy\\original_subtitles_2ce5/9.mp3\n",
      "Merging: D:/Work/Faculty/Masters/NLP/project/content/dummy\\original_subtitles_2ce5/10_before_pause.mp3\n",
      "Merging: D:/Work/Faculty/Masters/NLP/project/content/dummy\\original_subtitles_2ce5/10.mp3\n",
      "Merging: D:/Work/Faculty/Masters/NLP/project/content/dummy\\original_subtitles_2ce5/11_before_pause.mp3\n",
      "Merging: D:/Work/Faculty/Masters/NLP/project/content/dummy\\original_subtitles_2ce5/11.mp3\n",
      "Merging: D:/Work/Faculty/Masters/NLP/project/content/dummy\\original_subtitles_2ce5/12_before_pause.mp3\n",
      "Merging: D:/Work/Faculty/Masters/NLP/project/content/dummy\\original_subtitles_2ce5/12.mp3\n",
      "Merging: D:/Work/Faculty/Masters/NLP/project/content/dummy\\original_subtitles_2ce5/13_before_pause.mp3\n",
      "Merging: D:/Work/Faculty/Masters/NLP/project/content/dummy\\original_subtitles_2ce5/13.mp3\n",
      "Merging: D:/Work/Faculty/Masters/NLP/project/content/dummy\\original_subtitles_2ce5/14_before_pause.mp3\n",
      "Merging: D:/Work/Faculty/Masters/NLP/project/content/dummy\\original_subtitles_2ce5/14.mp3\n",
      "Merging: D:/Work/Faculty/Masters/NLP/project/content/dummy\\original_subtitles_2ce5/15_before_pause.mp3\n",
      "Merging: D:/Work/Faculty/Masters/NLP/project/content/dummy\\original_subtitles_2ce5/15.mp3\n",
      "Merging: D:/Work/Faculty/Masters/NLP/project/content/dummy\\original_subtitles_2ce5/16_before_pause.mp3\n",
      "Merging: D:/Work/Faculty/Masters/NLP/project/content/dummy\\original_subtitles_2ce5/16.mp3\n",
      "Input Sentence: But while we are fighting every day to build up our nation,\n",
      "Translated Sentence: Her gün kanepemde yürüyüşe ihtiyacımız var.\n",
      "Input Sentence: the far left only wants to wreck ruin and destroy our nation.\n",
      "Translated Sentence: Sadece Kuzey 5 vermek istiyorsan çalışmak için geçerken gittik.\n",
      "Input Sentence: And you know better than anybody.\n",
      "Translated Sentence: Sen ve biri senden daha iyi tanıyor mu?\n",
      "Input Sentence: For the last three years, Democrat lawmakers,\n",
      "Translated Sentence: Geçen yıl üç buçukta miydi?\n",
      "WARNING:tensorflow:5 out of the last 18 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001C1826B28E0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Input Sentence: their deep state cronies, the fake news media,\n",
      "Translated Sentence: Haberi duyduğunda, onun yanaklarından bastı.\n",
      "WARNING:tensorflow:5 out of the last 15 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001C182984040> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Input Sentence: they've been colluding in their effort to overturn\n",
      "Translated Sentence: Onlar onların kahramanıydı.\n",
      "Input Sentence: the presidential election.\n",
      "Translated Sentence: Başkan mutluluk planladı.\n",
      "Input Sentence: 63 million people voted and to nullify the votes of the American people.\n",
      "Translated Sentence: Amerikan bayrağının ve insanların müziğe tarihinde ihtiyaç duyar.\n",
      "Input Sentence: And many African-American people voted for Trump.\n",
      "Translated Sentence: Birçok kişi ve uçuş spor oy kullandı.\n",
      "Input Sentence: Even then, now they like me more because I said I was going to do it,\n",
      "Translated Sentence: Bana daha fazlasını söylemek için onu yaptım.\n",
      "Input Sentence: and now I did it, so you better vote.\n",
      "Translated Sentence: Ve bunu sana daha çok geç kaldım.\n",
      "Input Sentence: I said I was going to do it.\n",
      "Translated Sentence: Onu yapacağımı söyledim.\n",
      "Input Sentence: I said I'm going to do it.\n",
      "Translated Sentence: Onu yapacağımı söyledim.\n",
      "Input Sentence: Yeah?\n",
      "Translated Sentence: Evet, elbette.\n",
      "Input Sentence: Long live, long live, long live, long live, long live.\n",
      "Translated Sentence: Uzun uzun süre yaşamak uzun mesafe sürüyor.\n",
      "Input Sentence: Thank you, folks.\n",
      "Translated Sentence: Sana teşekkür ederiz.\n",
      "Translated SRT saved at: D:/Work/Faculty/Masters/NLP/project/content/dubbed_audio_translated.srt\n",
      "Dubbed video saved at: D:/Work/Faculty/Masters/NLP/project/content/dubbed_video.mp4\n"
     ]
    }
   ],
   "source": [
    "async def main(language=\"Turkish\", gender=\"Male\"):\n",
    "    audio_file_path = \"D:/Work/Faculty/Masters/NLP/project/Video_Dubbing/input files/trump_speech.mp4\"\n",
    "    output_video_path = \"D:/Work/Faculty/Masters/NLP/project/Video_Dubbing/content/dubbed_video.mp4\"\n",
    "    dub_save_path = \"D:/Work/Faculty/Masters/NLP/project/Video_Dubbing/content/dubbed_audio.mp3\"\n",
    "    txt_save_path = \"D:/Work/Faculty/Masters/NLP/project/Video_Dubbing/content/transcription_output.txt\"\n",
    "    srt_save_path = \"D:/Work/Faculty/Masters/NLP/project/Video_Dubbing/content/original_subtitles.srt\"\n",
    "\n",
    "    valid_languages = [\"Turkish\", \"Italian\", \"Spanish\", \"Dutch\"]\n",
    "    valid_genders = [\"Male\", \"Female\"]\n",
    "    \n",
    "    if language not in valid_languages:\n",
    "        raise ValueError(f\"Invalid language: {language}. Supported languages: {valid_languages}\")\n",
    "    if gender not in valid_genders:\n",
    "        raise ValueError(f\"Invalid gender: {gender}. Use 'Male' or 'Female'.\")\n",
    "\n",
    "    transcriber = AudioTranscriber()\n",
    "    tts = TextToSpeech()\n",
    "    dubber = SubtitleDubber(transcriber, tts)\n",
    "    video_dubber = VideoDubber()\n",
    "\n",
    "    srt_file_path, _ = transcriber.transcribe(audio_file_path, txt_save_path, srt_save_path)\n",
    "    await dubber.dub_srt(srt_file_path, dub_save_path, language, gender)\n",
    "    await video_dubber.dub_video(audio_file_path, dub_save_path, output_video_path)\n",
    "\n",
    "# Run with different languages and genders\n",
    "if __name__ == \"__main__\":\n",
    "    # Example 1: Turkish with Male voice (Seq2Seq)\n",
    "    await main(language=\"Dutch\", gender=\"Male\")\n",
    "\n",
    "    # Example 2: Spanish with Female voice (MarianMT)\n",
    "    # asyncio.run(main(language=\"Spanish\", gender=\"Female\"))\n",
    "\n",
    "    # Example 3: Italian with Male voice (MarianMT)\n",
    "    # asyncio.run(main(language=\"Italian\", gender=\"Male\"))\n",
    "\n",
    "    # Example 4: Dutch with Female voice (MarianMT)\n",
    "    # asyncio.run(main(language=\"Dutch\", gender=\"Female\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54bb3fba-b1f2-4881-b232-c822cea3110f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250436f3-a026-49c8-a893-db6b005c22ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ae39c1-a107-4c6e-8144-6f8be5db3c40",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
